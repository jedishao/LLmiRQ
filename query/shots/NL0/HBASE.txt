426	null hbase can&apos;t find remote filesystemIf filesystem is remote, e.g. its an Hadoop DFS running "over there", there is no means of pointing hbase at it currently (unless you count copying hadoop-site.xml into hbase/conf).  Should be possible to just set a fully qualified hbase.rootdir and that should be sufficient for hbase figuring the fs (needs to be backported to 0.1 too).
433	null region server should deleted restore log after successfull restoreCurrently we do not remove the restore log "oldlogfile.log" after we reopen a region after a crashed region server.Suggestion would be to remove after we successfully flush of all the edits to a mapfileso something like:replay logmemcache flushdeleted log
462	null Update migration toolHBASE-2 is really an incompatible change as it changes the format of region server log file names.Update Migration tool so that it ensures there are no unrecovered region server log files.
490	Client Region Doubly-assigned .META.; master uses one and clients anotherInternal cluster has two .META.,,1 regions up (Its possible for a region to be added twice to the unassigned map if meta scans run close together).  Worse is that the master is working with one .META. but when clients come in, they&amp;apos;re being give the other.  Makes for odd results.Made it a blocker.  Still trying to track down how master doesn&amp;apos;t see subsequent update of .META. info in ROOT.....
505	Region Region assignments should never time out so long as the region server reports that it is processing the open requestCurrently, when the master assigns a region to a region server, it extends the reassignment timeout when the region server reports that it is processing the open. This only happens once, and so if the region takes a long time to come on line due to a large set of transactions in the redo log or because the initial compaction takes a long time, the master will assign the region to another server when the reassignment timeout occurs.Assigning a region to multiple region servers can easily corrupt the region. For example:region server 1 is processing the redo log creating a new mapfile. It takes more than one interval to do so so the master assigns the region to region server 2. region server 2 starts processing the redo log creating essentially the same mapFile as region server 1, but with a different name.region server 2 can fail to open the region if region server 1 deletes the old log file or if it tries to open the new mapFile that region server 1 is creating.region server 1 can fail to open the region if it tries to open the mapFile that region server 2 is creating.Often region server 1 eventually succeeds and reports to the master that it has finished opening the region, but the master tells it to close that region because it has assigned it to another server. Region server 2 often fails to open the region, because the old log file has been deleted, or it fails to process the new map file created by region server 1.Proposed solution:During the open process the region server should send a MSG_PROCESS_OPEN with each heartbeat until the region is opened (when it sends MSG_REGION_OPEN). The master will extend the reassignment timeout with each MSG_PROCESS_OPEN it receives and will not assign the region to another server so long as it continues to receive heart beat messages from the region server processing the open.
528	null table &apos;does not exist&apos; when it doesThis one I&amp;apos;ve seen a few times.  In hql, I do show tables and it shows my table.  I then try to do a select against the table and hql reports table does not exist.  Digging, whats happening is that the getClosest facility is failing to find the first table region in the .META. table.  I hacked up a region reading tool  attached (for 0.1 branch)  and tried it against but a copy and the actual instance of the region and it could do the getClosest fine.  I&amp;apos;m pretty sure I restarted the HRS and when it came up again, the master had given it again the .META. and again was failing to find the first region in the table (Looked around in server logs and it seemed &amp;apos;healthy&amp;apos;).
589	null Remove references to deprecated methods in Hadoop once hadoop-0.17.0 is releasedA number of methods in Hadoop have been deprecated for release 0.17.0. Once 0.17.0 is released, use preferred alternate.
613	HStore Timestamp-anchored scanning fails to find all recordsIf I add 3 versions of a cell and then scan across the first set of added cells using a timestamp that should only get values from the first upload, a bunch are missing (I added 100k on each of the three uploads).  I thought it the fact that we set the number of cells found back to 1 in HStore when we move off current row/column but that doesn&amp;apos;t seem to be it.  I also tried upping the MAX_VERSIONs on my table and that seemed to have no effect.  Need to look closer.Build a unit test because replicating on cluster takes too much time.
620	null testmergetool failing in branch and trunk since hbase-618 went inThe hbase-618 fix revealed that testmergetool depends on compactions running.
624	null Master will shut down if number of active region servers is zero even if shutdown was not requestedThe master will initiate shutdown if the number of active region servers goes to zero, even if shutdown has not been requested.
674	Row memcache size unreliableMultiple updates against same row/column/ts will be seen as increments to cache size on insert but when we then play the memcache at flush time, we&amp;apos;ll only see the most recent entry and decrement the memcache size by whatever its size; memcache will be off.
697	null thrift idl needs update/edit to match new 0.2 API (and to fix bugs)Talking w/ Bryan, moving this out of the way of the 0.2.0 release.
698	null HLog recovery is not performed after master failureI have a local cluster running, and its logging to&lt;hbase&gt;/log_X.X.X.X_1213228101021_60020/Then I kill both master and regionserver, and restart. Looking throughthe logs I don&amp;apos;t see anything about trying to recover from this hlog,it just creates a new hlog alongside the existing one (with a newstartcode).  The older hlog seems to be ignored, and the tablescreated in the inital session are all gone.
703	Region RegionSplitter Invalid regions listed by regionserver.jspThe region list displayed by regionserver.jsp contains regions that have ceased existence due to splits.Example:Region Name	Encoded Name	Start Key	End Key...maxentriestest,acacdk,1214292085212	732557990 	acacdkmaxentriestest,acacdk,1214297936860	1583424516 	acacdk	acqtzkmaxentriestest,acacdk,1214293855954	1509492302 	acacdk	adhlxwmaxentriestest,acqtzk,1214297936862	1120286366 	acqtzk	adhlxwmaxentriestest,adhlxw,1214293855955	400707061 	adhlxwmaxentriestest,adhlxw,1214299372674	2060549477 	adhlxw	aelrxomaxentriestest,adhlxw,1214297324386	336026175 	adhlxw	afpxzsmaxentriestest,aelrxo,1214299372674	1352588233 	aelrxo	afpxzsmaxentriestest,afpxzs,1214297324387	1235754353 	afpxzs
737	Cell Row Get Scan Scanner: every cell in a row has the same timestampA row can have multiple cells, and each cell can have a different timestamp.  The get command in the shell demonstrates that cells are being stored with different timestamps:hbase(main):008:0&gt; get &amp;apos;table1&amp;apos;, &amp;apos;row2&amp;apos;COLUMN                       CELLfam1:letters                timestamp=1215707612949, value=deffam1:numbers                timestamp=1215707629064, value=123fam2:letters                timestamp=1215711498969, value=abc3 row(s) in 0.0100 secondsHowever, using the scanners to retrieve these cells shows that they all have the same timestamp:hbase(main):009:0&gt; scan &amp;apos;table1&amp;apos;ROW                          COLUMN+CELLrow2                        column=fam1:letters, timestamp=1215711498969, value=defrow2                        column=fam1:numbers, timestamp=1215711498969, value=123row2                        column=fam2:letters, timestamp=1215711498969, value=abc3 row(s) in 0.0600 secondsThe scanners are losing timestamp information somewhere along the line.
743	hbase bin/hbase migrate upgrade fails when redo logs existsI migrated several hbase-0.1.3 instances to hbase trunk and even if I stop hbase-0.1.3 cleanup it leaves redo logs on hdfs. The problems is that when migrating the data with hbase-trunk it fails because it finds these redo-logs and quit with a error message saying that we should reinstall the old hbase and shut it down cleanly and that in theory it erases the redo logs. The work around has been to delete the redo logs manually... which is bad.
819	classes methods Remove DOS-style ^M carriage returns from all code where foundThere are a few files that contain DOS-style carriage returns.  This is leading to issues when applying patches.The presence of these may also be causing a snowball effect as some IDEs/editors may see one and attempt to apply that LF/CR format to all lines or files.
848	Cell API to inspect cell deletionsIf a cell gets deleted, I&amp;apos;d like to have some API that gives me the deletion timestamp, as well as any versions that predate the deletion.One possibility might be to add a boolean flag to HTable.get
865	classes methods Fix javadoc warningsThere are javadoc warnings in both the 0.2 branch and in trunk. They must be fixed before 0.2.2 or 0.18.0 are released.
871	Table Major compaction periodicity should be specifyable at the column family level, not cluster widejon gray has a table of ten rows and a couple of columns that is constantly being updated.  Has max versions of 2.  This table is growing fast because all versions written are kept until a major compaction.  The way this table is being used is different than that of others.  Would be good if he could have major compactions run more often than the default once a day.
889	null The current Thrift API does not allow a new scanner to be created without supplying a column list unlike the other APIs.The current Thrift API does not allow a new scanner to be created without supplying a column list, unlike the REST api. I posted this on the HBase-Users mailing list. Others concurred that it appears to have been an oversight in the Thrift API.Its quite significant as there is no easy work around, unless you already know which the column families names then list them all when you open the scanner.
900	Table Regionserver memory leak causing OOME during relatively modest bulk importingI have recreated this issue several times and it appears to have been introduced in 0.2.During an import to a single table, memory usage of individual region servers grows w/o bounds and when set to the default 1GB it will eventually die with OOME.  This has happened to me as well as Daniel Ploeg on the mailing list.  In my case, I have 10 RS nodes and OOME happens w/ 1GB heap at only about 30-35 regions per RS.  In previous versions, I have imported to several hundred regions per RS with default heap size.I am able to get past this by increasing the max heap to 2GB.  However, the appearance of this in newer versions leads me to believe there is now some kind of memory leak happening in the region servers during import.
921	Region Client region close and open processed out of order; makes for disagreement between master and regionserver on region stateMaster assigns region X successfully.  It then decides to close it because it wants it opened elsewhere as part of region rebalancing.  Both the open and close operations are reported back to the master.  Both have operation processing components that are added to the todo list to be processed in another thread outside of the master&amp;apos;s main loop.The close operation does the bulk of its work inline with the master main processing loop.  Its todo component does some work if the region is offlined but otherwise nothing of consequence whereas the open in its todo does the important meta catalog table update with the new location information.Its been fairly common here on our cluster where the master todo queue is occupied processing the shutdown of a regionserver.  It takes a long time to process the shutdown of a regionserver when thousands of regions   This latter delays the processing of the open and close todos.  In effect the open is running after the close.  The region goes into limbo.  Only a restart of the &amp;apos;hosting&amp;apos; regionserver &amp;apos;fixes&amp;apos; this state.This is a particular case of the general HBASE-543 issue.  Its happening alot here on our cluster so will hack up a fix for this and get it into TRUNK and backport it to 0.18.1.Jim Firby here had a good idea for conditions like this.  Clients should be able to say "I&amp;apos;ve asked for a regions location 10 times now and Mr. Master, you&amp;apos;ve given me the same response ten times in a row and each time, the answer was wrong.  Revisit any notion that said region is at said location".  Mr. Master would then go off and do something drastic like close and reassign the region.
938	Region major compaction period is not checked periodicallyThe major compaction period, hbase.hregion.majorcompaction, is not checked periodically. Currently, we only request major compaction when the region is open or split at which point we check whether the major compaction period is due.
994	Cluster IPC interfaces with different versions can cause problemsThis morning we ran into a situation in which some 0.2.x region servers started up and joined a 0.18.1 cluster.This &amp;apos;sort of&amp;apos; worked in that the hrs could communicate with the master, but clients could not communicate with the 0.2 region servers because the api version changed (the master wouldn&amp;apos;t have liked it if it had deployed root or meta there).Suggestion is that we have a single api version that gets bumped when any of the interfaces changes.
1142	Server Cleanup thrift server; remove Text and profuse DEBUG messagingAmbiguous issue name.. sorry.The thrift server has loads of getText(..) calls. Which is a local function that checks for utf8 compliance, we don&amp;apos;t need them anywhere, because we don&amp;apos;t use Text anymore.There is probably other things we missed last time we updated the api, that we should also clean up while we&amp;apos;re at it. Open to suggestions.
1157	null If we do not take start code as a part of region server recovery, we could inadvertantly try to reassign regions assigned to a restarted server with a different start code
1185	null wrong request/sec in the gui reporting wrongI am seeing lower number of request in the masters gui then I have seen in 0.18.0 while scanning.I thank part of it is we moved to report per sec request not per 3 secs so the request should be 1/3 of the old numbers I was getting.hbase.client.scanner.caching is not the reason the request are under reported.I set hbase.client.scanner.caching = 1 and still get about 2K request a sec in the guibut when the job is done I take records / job time and get 36,324/ records /sec. Sothere must be some caching out side of the hbase.client.scanner.caching making therequest per sec lower then it should be. I know it running faster then reported just thoughtit might give some new users the wrong impression that request/sec = read/write /sec.
1187	Table HBase After disabling/enabling a table, the regions seems to be assigned to only 1-2 region serversAfter disabling/enabling a small table (20 regions), we see that the master tend to assign the regions to only 1-2 region servers. Unfortunately, that table is extensively used in random reads which really kills those RS when they hold those regions. As a fix, we have to restart HBase...
1267	Table binary keys broken in trunk (again).Binary keys, specifically ones where the first byte of the key is nul &amp;apos;\0&amp;apos; don&amp;apos;t work:Splits happenLogfile indicates everything normalBut the .META. doesnt list all the regions.  It only lists the &amp;apos;basic&amp;apos; regions: &amp;apos;table,,1234&amp;apos;.  The other regions with the binary keys in the middle just dont seem to be in .META....
1279	HBase Fix the way hostnames and IPs are handledFrom the list by Yabo-Arber Xu,Yes, I&amp;apos;ve unlocked the port, and i am actually able to access from the webUI with a client not running on EC2 to HBase at example.com:60010. It showsall User Tables, but the Region Servers Address is the EC2 internal address:domU-12-31-39-00-65-E5.compute-1.internal:60020.I guess the client fails because it can not connect region server, whichserves only for an internal IP. However, in hbase-site.xml, I did configurewith region server explicitly in its external IP.&lt;property&gt;&lt;name&gt;hbase.regionserver&lt;/name&gt;&lt;value&gt;ec2-67-202-57-127.compute-1.amazonaws.com:60020&lt;/value&gt;&lt;description&gt;The host and port a HBase region server runs at.&lt;/description&gt;&lt;/property&gt;In fact we completely bypass the hostname set in hbase.regionserver, also the hostnames in the web UI are not the good ones. We should do that part like hadoop does.
1290	null table.jsp either 500s out or doesnt list the regionsThe table.jsp page either 500 errors out if you are viewing a .META. or ROOT table, or for user tables it doesn&amp;apos;t list the regions.
1318	ThriftServer Thrift server doesnt know about atomicIncrementthe thrift server needs the atomicIncrement API implemented
1457	null Taking down ROOT/META regionserver can result in cluster becoming in-operationalTake down a regionserver via controlled or uncontrolled shutdown, the master doesn&amp;apos;t properly reassign the root/meta regions.
1466	region Binary keys are not first class citizensIf you use binary keys, you don&amp;apos;t get full features as if you were not using binary keys.  Some things that are broken:grep/less cant search in logfiles with binary datadisplays are unreadable due to weird utf8/other issuescan&amp;apos;t use the region historianetc
1485	null Wrong or indeterminate behavior when there are duplicate versions of a columnAs of now, both gets and scanners will end up returning all duplicate versions of a column.  The ordering of them is indeterminate.We need to decide what the desired/expected behavior should be and make it happen.Note:  It&amp;apos;s nearly impossible for this to work with Gets as they are now implemented in 1304 so this is really a Scanner issue.  To implement this correctly with Gets, we would have to undo basically all the optimizations that Gets do and making them far slower than a Scanner.
1545	null atomicIncrements creating new values with Long.MAX_VALUEAtomic increment is creating new key values with timestamp of Long.MAX_VALUE.  This is not good, makes it hard to do range queries (as most of Thrift queries are).
1547	hregion atomicIncrement doesnt increase hregion.memcacheSizethis prevents flushing!
1558	HConstants deletes use &apos;HConstants.LATEST_TIMESTAMP&apos; but no one translates that into &apos;now&apos;Deletes don&amp;apos;t update MAX_TIMESTAMP -&gt; now like puts do.
1574	Client Server Client and server APIs to do batch deletes.in 880 there is no way to do a batch delete (anymore?).  We should add one back in.
1576	null TIF needs to be able to set scanner caching size for smaller row tables & performanceTIF goes with the default scanner caching size (1).  When each row is processed very fast and is small, this limits the overall performance.  By setting a higher scanner caching level you can achieve 100x+ the performance with the exact same map-reduce and table.
1597	Storefile Prevent unnecessary caching of blocks during compactionsWhen running any kind of compaction, we read every block of every storefile being compacted into the block cache.We would like to reuse any already cached blocks, if available, but otherwise we should not bog down the LRU with unnecessary blocks.This is not as bad as it was with the old LRU because the latest LRU implementation (HBASE-1460) is scan-resistant.  This ensures that we are not causing massive eviction of the blocks that are being read multiple times or from in-memory tables.  However, this does add to the GC-woes of an import because each block gets further referenced, and for longer periods of time.  There is also overhead in running the LRU evictions.
1659	null merge tool doesnt take binary regions with \x escape formatas per short
1714	Scan convenience functions in Scan and the thrift API along with a few other bug fixesa number of handy things i&amp;apos;ve added to my own repo recently
1784	null Missing rows after medium intensity insertThis bug was uncovered by Mathias in his mail "Issue on data load with 0.20.0-rc2". Basically, somehow, after a medium intensity insert a lot of rows goes missing. Easy way to reproduce : PE. Doing a PE scan or randomRead afterwards won&amp;apos;t uncover anything since it doesn&amp;apos;t bother about null rows. Simply do a count in the shell, easy to test (I changed my scanner caching in the shell to do it faster).I tested some light insertions with force flush/compact/split in the shell and it doesn&amp;apos;t break.
1792	HBase Table [Regression] Cannot save timestamp in the future0.20, compared to previous versions, doesn&amp;apos;t let you save with a timestamp in the future and will set it to current time without telling you. This is really bad for users upgrading to 0.20 that were using those timestamps.Example:hbase(main):004:0&gt; put &amp;apos;testtable&amp;apos;, &amp;apos;r1&amp;apos;, &amp;apos;f1:c1&amp;apos;, &amp;apos;val&amp;apos;, 53739653353369111680 row(s) in 0.0070 secondshbase(main):005:0&gt; scan &amp;apos;testtable&amp;apos;ROW                          COLUMN+CELLr1                          column=f1:c1, timestamp=1251223892010, value=val1 row(s) in 0.0380 seconds
1794	Store Region recovered log files are not inserted into the storefile mapafter a log recovery, the resulting flushed file is not introduced into the store.storefiles map. The new data isnt available until the region is closed or compacted.
1798	Delete [Regression] Unable to delete a row in the futureDeleting in the future doesn&amp;apos;t work because KV resets everything to now.
1921	Cluster When the Master&apos;s session times out and there&apos;s only one, cluster is wedgedOn IRC, some fella had a session expiration on his Master and had only one. Maybe in this case the Master should first try to re-get the znode?
3816	null Put the balancer switch in ZKCurrently the balancer switch lives only in the master&amp;apos;s memory. Yesterday we had one master that died (in a strange way) and the one we restarted started balancing again. It&amp;apos;s only a test cluster so it really wasn&amp;apos;t a big deal, but I can imagine ways this could screw people&amp;apos;s life.
3885	null Remove hbase.regionserver.flushlogentries, its not used
3939	Server Response Client Some crossports of Hadoop IPC fixesA few fixes from Hadoop IPC that we should probably cross-port into our copy:HADOOP-7227: remove the protocol version check at call timeHADOOP-7146: fix a socket leak in serverHADOOP-7121: fix behavior when response serialization throws an exceptionHADOOP-7346: send back nicer error response when client is using an out of date IPC version
4100	HBase server HBase Authentication for REST clientsLike Thrift, the REST gateway is not currently integrated into the authentication used for HBase RPC.  Currently this means the REST gateway cannot even be used when HBase security is active.For the REST gateway to be able to interoperate with HBase security:the REST server needs to be able to login from a keytab on startup with its own server principalREST clients need to be able to authenticate security with the REST serverthe REST server needs to be able to act as a trusted proxy for the original client identities, so that the HBase authorization checks can be performed against the original client requestLike Thrift, implementing step #1 as a bare minimum would at least allow deploying a REST server configured to login as the application user on startup.  Even without authenticating REST clients, this would allow the gateway to work when HBase security is active.For step #2, we can make use of SPNEGO to provide Kerberos/GSSAPI authentication of clients over HTTP.  The Alfredo library from Cloudera would hopefully make this relatively easy to do:http://cloudera.github.com/alfredo/docs/latest/index.html
5062	SecurityUtil HBaseKerberosUtils HBaseConfiguration Missing logons if security is enabledSomehow the attached changes are missing from the security integration.
5372	Table UserPermission AccessController PermissionStorage Table mutation operations should check table level rights, not global rightsdrop/modify/disable/enable etc table operations should not check for global CREATE/ADMIN rights, but table CREATE/ADMIN rights. Since we check for global permissions first for table permissions, configuring table access using global permissions will continue to work.
5640	HBase bulk load runs slowly than beforeI am loading data from an external system into hbase. There are many prints of the form. This is possibly a regression caused by a recent patch.....on different filesystem than destination store - moving to this filesystem
5714	Hbck Add write permissions check before any hbck run that modifies hdfs.We encoutered a situation where hbck was run by an under-privileged user that was unable to write/modify/merge regions due to hdfs perms.  Unfortunately, this user was alerted of this  after several minutes of read-only operations.  hbck should fail early by having a write perm check and providing actionable advice to the hbase admin.Maybe something like: "Current user yy does not have write perms to &lt;hbase home&gt;. Please run hbck as hdfs user xxx"
5820	Hbck HBase hbck should check fs owner/permissions.In some cases, hbck needs to be run as a user that has write perms to the file system.  If it writes data to hbase&amp;apos;s directories, it may write new files/dirs that the hbase processes&amp;apos;s user does not have permissions to.  We should alert the user of this situation.
6166	ThriftServer Allow thrift to start wih the server type specified in configCurrently the thrift server type must be specified on the command line.  If it&amp;apos;s already in config it shouldn&amp;apos;t be needed.
6446	ReplicationSource Replication source will throw EOF exception when hlog size is 0when master cluster startup new hlog which size is 0 will be created. if we start replication, replication source will print many EOF exception when openreader. I think we need to ignore this case and do not print so many exception warning log .
7825	classes methods Retire non distributed log splitting related codeI think we only use distributed log splitting now and the legacy code before distributed log splitting should be retired. Any objections?Thanks,-Jeffrey
8258	hbase Make mapreduce tests pass on hadoop2HBASE-7904 was a first attempt at making this work but it got lost in the weeds.This is a new attempt at making hbase mapreduce jobs run on hadoop2 (w/o breaking mapreduce on hadoop1)
8962	classes methods Clean up code and remove regular log splittingDistributed log splitting has been out there for a while and it is kind of stable. It&amp;apos;s about time to get rid of the regular log splitting and clean up the code a little bit.
10169	null Batch coprocessorThis is designed to improve the coprocessor invocation in the client side.Currently the coprocessor invocation is to send a call to each region. If theres one region server, and 100 regions are located in this server, each coprocessor invocation will send 100 calls, each call uses a single thread in the client side. The threads will run out soon when the coprocessor invocations are heavy.In this design, all the calls to the same region server will be grouped into one in a single coprocessor invocation. This call will be spread into each region in the server side.
10622	ExportSnapshot Improve log and Exceptions in Export Snapshotfrom the logs of export snapshot is not really clear what&amp;apos;s going on,adding some extra information useful to debug, and in some places the real exception can be thrown
11222	classes methods Add integration test for visibility deletesOnce HBASE-10885 gets checked in, create integration tests to verify the same.
12178	Region Failed splits are kept in transitionWhen a region split fails ( took too long to write the reference files ) the region stays int SPLITTING_NEW state and in transition on the master forever. This will block balancer invocations.
12373	VisibilityController VisibilityLabelsCache Get Result Cell KeyValue Bytes VisibilityClient HBaseTestingUtility Provide a command to list visibility labelsA command to list visibility labels that are in place would be handy.This is also in line with many of the other hbase list commands.
12469	User Way to view current labelsThere is currently no way to get the available labels for a system even if you are the super user.  You have to run a scan of hbase:labels and then interpret the output.
12770	ReplicationSource Don&apos;t transfer all the queued hlogs of a dead server to the same alive serverWhen a region server is down(or the cluster restart), all the hlog queues will be transferred by the same alive region server. In a shared cluster, we might create several peers replicating data to different peer clusters. There might be lots of hlogs queued for these peers caused by several reasons, such as some peers might be disabled, or errors from peer cluster might prevent the replication, or the replication sources may fail to read some hlog because of hdfs problem. Then, if the server is down or restarted, another alive server will take all the replication jobs of the dead server, this might bring a big pressure to resources(network/disk read) of the alive server and also is not fast enough to replicate the queued hlogs. And if the alive server is down, all the replication jobs including that takes from other dead servers will once again be totally transferred to another alive server, this might cause a server have a large number of queued hlogs(in our shared cluster, we find one server might have thousands of queued hlogs for replication). As an optional way, is it reasonable that the alive server only transfer one peer&amp;apos;s hlogs from the dead server one time? Then, other alive region servers might have the opportunity to transfer the hlogs of rest peers. This may also help the queued hlogs be processed more fast. Any discussion is welcome.
12813	null Reporting region in transition shouldn&apos;t loop foreverWe had an issue where a region server wasn&amp;apos;t able to send the report region in transition request. Well after failing it just retries forever.At some point it would have been better to just abort the region server if it can&amp;apos;t talk to master.
13647	HBase Client Operation Coprocessor Default value for hbase.client.operation.timeout is too highDefault value for hbase.client.operation.timeout is too high, it is LONG.Max.That value will block any service calls to coprocessor endpoints indefinitely.Should we introduce better default value for that?
13737	Snapshot Table Scan Client [HBase MOB] MOBTable cloned from a snapshot leads to data loss, when that actual snapshot and main table is deleted.clone snapshot on mob feature leads to data loss for mob data.steps to reproduce:===============1. created MOB table with two column families like "mobcf" (mob is enabled) and "norcf"2. insert mob data and normal data at a time into the table.3. scan the MOB table.4. take snapshot by specifying the table name.5. clone the snapshot by specifying the new table name.6. check the new table is created or not and try scan for the new table which have both mob data and normal data should give back to the client.7. delete the snapshot which done in step 4.8. delete the main table which done in step 1.9. Now scan the new table again.
13821	BucketCache WARN if hbase.bucketcache.percentage.in.combinedcache is setHBASE-11520 improved configuration of bucket cache to no longer require hbase.bucketcache.percentage.in.combinedcache. This was done rather aggressively, with this previously mandatory configuration being ignored. This can result in RS crashes for unsuspecting users. We should add a WARN when hbase.bucketcache.percentage.in.combinedcache is set to make debugging the crash more straight forward.
13826	User Table Unable to create table when group acls are appropriately set.Steps for reproducing the issue.Create user &amp;apos;test&amp;apos; and group &amp;apos;hbase-admin&amp;apos;.Grant global create permissions to &amp;apos;hbase-admin&amp;apos;.Add user &amp;apos;test&amp;apos; to &amp;apos;hbase-admin&amp;apos; group.Create table operation for &amp;apos;test&amp;apos; user will throw ADE.
13863	null Multi-wal feature breaks reported number and size of HLogsWhen multi-wal is enabled the number and size of retained HLogs is always reported as zero.We should fix this so that the numbers are the sum of all retained logs.
13865	classes Increase the default value for hbase.hregion.memstore.block.multipler from 2 to 4 (part 2)Its 4 in the book and 2 in a current master.
13885	Snapshot ZKWatcher Table ZK watches leaks during snapshotsWhen taking snapshot of a table a watcher over /hbase/online-snapshot/abort/snapshot-name is created which is never cleared when the snapshot is successful. If we use snapshots to take backups daily we accumulate a lot of watches.Steps to reproduce -1) Take snapshot of a table - snapshot &amp;apos;table_1&amp;apos;, &amp;apos;abc&amp;apos;2) Run the following on zk node or alternatively observe zk watches metricecho "wchc" | nc localhost 2181/hbase/online-snapshot/abort/abc can be found.
13895	Region WAL DATALOSS: Region assigned before WAL replay when abortOpening a place holder till finish analysis.I have dataloss running ITBLL at 3B (testing HBASE-13877). Most obvious culprit is the double-assignment that I can see.
14092	HBCK hbck should run without locks by default and only disable the balancer when necessaryHBCK is sometimes used as a way to check the health of the cluster. When doing that it&amp;apos;s not necessary to turn off the balancer. As such it&amp;apos;s not needed to lock other runs of hbck out.We should add the --no-lock and --no-balancer command line flags.
14094	null Procedure.proto can&apos;t be compiled to C++EOF is a defined symbol in c and C++.
14098	HFile StoreFile CompactionRequest CompactionContext HStore BlockCache Allow dropping caches behind compactions
14166	Region Per-Region metrics can be staleWe&amp;apos;re seeing some machines that are reporting only old region metrics. It seems like at some point the Hadoop metrics system decided which metrics to display and which not to. From then on it was not changing.
14208	classes methods Remove yarn dependencies on -common and -clientThey aren&amp;apos;t really needed since MR can&amp;apos;t be used without server.
14224	Coprocessor Fix coprocessor handling of duplicate classesWhile discussing with Misty Stanley-Jones over on HBASE-13907 we noticed some inconsistency when copros are loaded. Sometimes you can load them more than once, sometimes you can not. Need to consolidate.
14597	null Fix Groups cache in multi-threaded envUGI doesn&amp;apos;t hash based on the user as expected so since we have lots of ugi potentially created the cache doesn&amp;apos;t do it&amp;apos;s job.
14723	Table Fix IT tests split too many timesSplitting the whole table is happening too often. Lets make this happen less frequently as there are more regions.
14755	classes methods Fix some broken links and HTML problemsProblems seen in https://builds.apache.org/view/H-L/view/HBase/job/HBase%20Website%20Link%20Ckecker/3/artifact/link_report/index.html
14778	BlockCache Make block cache hit percentages not integer in the metrics systemOnce you&amp;apos;re close to the 90%+ it&amp;apos;s hard to see a difference because getting a full percent change is rare.
14781	HBaseConfiguration FlushLargeStoresPolicy Turn per cf flushing on for ITBLL by default
14788	Region BucketCache Splitting a region does not support the hbase.rs.evictblocksonclose config when closing source regioni have a table with bucket cache turned on and hbase.rs.evictblocksonclose set to false.  I split a region and watched that the closing of the source region did not complete until the bucketcache was flushed for that region.
14793	null Allow limiting size of block into L1 block cache.G1GC does really badly with long lived large objects. Lets allow limiting the size of a block that can be kept in the block cache.
14822	LeaseManager HRegionServer ScanInfo ResultScanner Renewing leases of scanners doesn&apos;t work
14840	Table Sink cluster reports data replication request as success though the data is not replicatedScenario:Sink cluster is downCreate a table and enable table replicationPut some dataNow restart the sink clusterObservance:Data is not replicated in sink cluster but still source cluster updates the WAL log position in ZK, resulting in data loss in sink cluster.
14983	BlockType Create metrics for per block type hit/miss ratiosMissing a root index block is worse than missing a data block. We should know the difference
15098	null Normalizer switch in configuration is not usedThe newly added global switch to enable the new normalizer functionality is never used apparently, meaning it is always on. The hbase-default.xml has this:&lt;property&gt;&lt;name&gt;hbase.normalizer.enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;description&gt;If set to true, Master will try to keep region sizewithin each table approximately the same.&lt;/description&gt;&lt;/property&gt;But only a test class uses it to set the switch to "true". We should implement a proper if statement that checks this value and properly disables the feature cluster wide if not wanted.
15146	queue Don&apos;t block on Reader threads queueing to a scheduler queueBlocking on the epoll thread is awful. The new rpc scheduler can have lots of different queues. Those queues have different capacity limits. Currently the dispatch method can block trying to add the the blocking queue in any of the schedulers.This causes readers to block, tcp acks are delayed, and everything slows down.
15278	Connection AsyncRPCClient hangs if Connection closes before RPC call responseThe test for HBASE-15212 discovered an issue with Async RPC Client.In that test, we are closing the connection if an RPC call writes a call larger than max allowed size, the server closes the connection. However the async client does not seem to handle connection closes with outstanding RPC calls. The client just hangs.Marking this blocker against 2.0 since it is default there.
15297	HBase error message is wrong when a wrong namspace is specified in grant in hbase shellIn HBase shell, specify a non-existing namespace in "grant" command, such ashbase(main):001:0&gt; grant &amp;apos;a1&amp;apos;, &amp;apos;R&amp;apos;, &amp;apos;@aaa&amp;apos;    &lt;--- there is no namespace called "aaa"The error message issued is not correctERROR: Unknown namespace a1!a1 is the user name, not the namespace.The following error message would be betterERROR: Unknown namespace aaa!orCan&amp;apos;t find a namespace: aaa
15315	HMaster Remove always set super user call as high priorityCurrent implementation set superuser call as ADMIN_QOS, but we have many customers use superuser to do normal table operation such as put/get data and so on. If client put much data during region assignment, RPC from HMaster may timeout because of no handle. so it is better to remove always set super user call as high priority.
15424	HFile WAL Replication Add bulk load hfile-refs for replication in ZK after the event is appended in the WALCurrenlty hfile-refs znode used for tracking the bulk loaded data replication is added first and then the bulk load event in appended in the WAL. So this may lead to a issue where the znode is added in ZK but append to WAL is failed(due to some probelm in DN), so this znode will be left in ZK as it is and will not allow hfile to get deleted from archive directory.
15441	WAL Region SplitLogWorker Fix WAL splitting when region has moved multiple timesCurrently WAL splitting is broken when a region has been opened multiple times in recent minutes.Region open and region close write event markers to the wal. These markers should have the sequence id in them. However it is currently getting 1. That means that if a region has moved multiple times in the last few mins then multiple split log workers will try and create the recovered edits file for sequence id 1. One of the workers will fail and on failing they will delete the recovered edits. Causing all split wal attempts to fail.We need to:make sure that close get the correct sequence id for open.Filter all region events from recovered editsIt appears that the close event with a sequence id of one is coming from region warm up.
15504	null Fix Balancer in 1.3 not moving regions off overloaded regionserverWe pushed 1.3 to a couple of clusters. In some cases the regions were assigned VERY un-evenly and the regions would not move after that.We ended up with one rs getting thousands of regions and most servers getting 0. Running balancer would do nothing. The balancer would say that it couldn&amp;apos;t find a solution with less than the current cost.
15635	null Mean age of Blocks in cache (seconds) on webUI should be greater than zero
15637	Server Server Call Queue TSHA Thrift-2 server should allow limiting call queue sizeRight now seems like thrift-2 hsha server always create unbounded queue, which could lead to OOM)
15669	ReplicationSource HFile Cell HFile size is not considered correctly in a replication requestIn a single replication request from source cluster a RS can send either at most replication.source.size.capacity size of data or replication.source.nb.capacity entries.The size is calculated by considering the cells size in each entry which will get calculated wrongly in case of bulk loaded data replication, in this case we need to consider the size of hfiles not cell.
15952	user hbase client replication Permission Bulk load data replication is not working when RS user does not have permission on hfile-refs nodeIn our recent testing in secure cluster we found that when a RS user does not have permission on hfile-refs znode then RS was failing to replicate the bulk loaded data as the hfile-refs znode is created by hbase client and RS user may not have permission to it.
16062	WAL WAL Improper error handling in WAL Reader/Writer creationIf creation of WAL reader/ writer fails for some reason RS may leak hanging socket in CLOSE_WAIT state.
16110	WAL AsyncFS WAL doesn&apos;t work with Hadoop 2.8+The async wal implementation doesn&amp;apos;t work with Hadoop 2.8+. Fails compilation and will fail running.
16144	null Replication queue&apos;s lock will live forever if RS acquiring the lock has died prematurelyIn default, we will use multi operation when we claimQueues from ZK. But if we set hbase.zookeeper.useMulti=false, we will add a lock first, then copy nodes, finally clean old queue and the lock.However, if the RS acquiring the lock crash before claimQueues done, the lock will always be there and other RS can never claim the queue.
16221	server Connection Connection Scan Operation Thrift server drops connection on long scansThrift servers use connection cache and we drop connections after hbase.thrift.connection.max-idletime milliseconds from the last time a connection object was accessed. However, we never update this last accessed time on scan path.By default, this will cause scanners to fail after 10 minutes, if the underlying connection object is not being used along other operation paths (like put).
16581	WAL Optimize Replication queue transfers after server fail overCurrently if a region server fails, the replication queue of this server will be picked up by another region server.  The problem is this queue can possibly be huge and contains queues from other multiple or cascading server failures.We had such a case in production.  From zk_dump:.../hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1467603735059: 18748267/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1471723778060:/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1468258960080:/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1468204958990:/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1469701010649:/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1470409989238:/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1471838985073:/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1467142915090: 57804890/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1472181000614:/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1471464567365:/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1469486466965:/hbase/replication/rs/r01data10-va-pub.xxx.ext,60020,1472680007498/1-r01data10-va-pub.xxx.ext,60020,1455993417125-r01data07-va-pub.xxx.ext,60020,1472680008225-r01data08-va-pub.xxx.ext,60020,1472680007318/r01data10-va-pub.xxx.ext%2C60020%2C1455993417125.1467787339841: 47812951...There were hundreds of wals hanging under this queue, coming from diferent region servers, which took a long time to replicate.We should have a better strategy which lets live region servers each grep part of this nested queue, and replicate in parallel.
16614	null Use daemon thread for netty event loopAs always use daemon thread in rpc implementation.
16711	null Fix hadoop-3.0 profile compileThe -Dhadoop.profile=3.0 build is failing currently due to code deprecated in hadoop2 and removed in hadoop3.
