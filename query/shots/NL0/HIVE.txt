218	method method method method method method method method predicate on partitioning column is ignored in many placesWe tried two queries yesterday that bought up several problems:1. predicate on partitioning column within a join clause was ignored:FROM (FROM xxx a SELECT a.xx, a.yy, a.ds WHERE a.ds=2009-01-05 UNION ALL FROM yyy SELECT b.xx, b.yy, b.ds WHERE b.ds=2009-01-05 UNION ALL FROM zzz c SELECT c.xx, c.yy, c.ds WHERE c.ds=2009-01-05) d JOIN aaa e ON (d.xx=e.xx AND e.ds=2009-01-05) INSERT OVERWRITE TABLE ...the plan tried to scan all partitions!2. predicate on partitioning clause inside insert clause was ignored (we took the previous query and moved the partition filter to the insert statement)FROM (FROM xxx a SELECT a.xx, a.yy, a.ds WHERE a.ds=2009-01-05 UNION ALL FROM yyy SELECT b.xx, b.yy, b.ds WHERE b.ds=2009-01-05 UNION ALL FROM zzz c SELECT c.xx, c.yy, c.ds WHERE c.ds=2009-01-05) d JOIN aaa e ON (d.xx=e.xx ) INSERT OVERWRITE TABLE ... WHERE e.ds=2009-01-05;the plan again tried to scan all partitionsthe really bad thing is that we were able to detect this problem only because of metastore inconsistencies - otherwise - we would have merrily scanned all the data. This is really critical to get fixed - because this means that we may actually be scanning tons of unnecessary data in production.
245	UDF Add POW(X, Y) UDFSee http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_pow
251	SELECT Hive Failures in Transform don&apos;t stop the jobIf the program executed via a SELECT TRANSFORM() USING &amp;apos;foo&amp;apos; exits with a non-zero exit status, Hive proceeds as if nothing bad happened.  The main way that the user knows something bad has happened is if the user checks the logs (probably because he got no output).  This is doubly bad if the program only fails part of the time (say, on certain inputs) since the job will still produce output and thus the problem will likely go undetected.
268	Table User "Insert Overwrite Directory" to accept configurable table row formatThere is no way for the users to control the file format when they are outputting the result into a directory.We should allow:INSERT OVERWRITE DIRECTORY "/user/zshao/result"ROW FORMAT DELIMITEDFIELDS TERMINATED BY &amp;apos;9&amp;apos;SELECT tablea.* from tablea;
269	UDF Add log/exp UDF functions to HiveSee http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.htmlEXP() 	Raise to the power ofLN() 	Return the natural logarithm of the argumentLOG10() 	Return the base-10 logarithm of the argumentLOG2() 	Return the base-2 logarithm of the argumentLOG() 	Return the natural logarithm of the first argumentPOW() 	Return the argument raised to the specified powerPOWER() 	Return the argument raised to the specified power
287	Column Table support count(*) and count distinct on multiple columnsThe following query does not work:select count(distinct col1, col2) from Tbl
305	Status Port Hadoop streaming&apos;s counters/status reporters to Hive Transformshttps://issues.apache.org/jira/browse/HADOOP-1328" Introduced a way for a streaming process to update global counters and status using stderr stream to emit information. Use "reporter:counter:&lt;group&gt;,&lt;counter&gt;,&lt;amount&gt; " to update  a counter. Use "reporter:status:&lt;message&gt;" to update status. "
308	FileSinkOperator UnionOperator UNION ALL should create different destination directories for different operandsThe following query hangs:select * from (select 1 from zshao_lazy union all select 2 from zshao_lazy) a;The following query produce wrong results: (one map-reduce job overwrite/cannot overwrite the result of the other)select * from (select 1 as id from zshao_lazy cluster by id union all select 2 as id from zshao_meta) a;The reason of both is that the destination directory of the file sink operator conflicts with each other.
323	CliDriver SessionState row counts for one query are being in printed subsequent querieswhen executing multiple queries from the cli - i am seeing the row count state being maintained/printed across queries:&gt;q1N1 rows&gt;q2N1 rows insertedN2 rows inserted
325	Partition [Hive] rand() should be ignored by input pruningselect * from T where rand() &lt; 0.5may return 0 rows because all partitions may simply be eliminated by partition pruning if rand() &lt; 0.5 happens to be false
326	Hive Select [hive] groupby count distinct with nulls has some problemsselect a, count(distinct b) from T group by a;had some problems if b is null.I will construct the exact testcase and get back
327	SessionState HiveQueryResultSet HiveStatement HiveSessionImpl row count getting printed wronglyWhen multiple queries are executed in same session, row count of the first query is getting printed for subsequent queries.
338	File Executing cli commands into thrift serverLet thrift server support set, add/delete file/jar and normal HSQL query.
342	null TestMTQueries is brokenIt has been broken for quite sometime but the build is not failing.
363	Reducer SelectOperator [hive] extra rows for count distinctselect count(distinct a) from Treturns dummy rows from all reducers if number of reducers are more than 1
367	GroupByOperator HiveInputFormat HiveConf RecordReader TableScanOperator ReduceSinkOperator [hive] problem in group by in case of empty input files
373	Reducer [hive] 1 reducer should be used if no grouping key is present in all scenarios
386	null Streaming processes should be able to return successfully without consuming all their input data.Currently if a streaming process exits without consuming all data in stdin, it causes a java IOException with Broken pipe. It seems like it should be possible to distinguish between broken pipe and the actual return code of the sub-process; so that broken pipe by itself  should be caught and ignored if the sub-process produced a SUCCESS return code.
403	null remove password password params from job config that is submitted to job trackerDo not show metastore db password when it is sent to job tracker and do not print this option in logs.
419	Table HdfsUtils Rename HDFS directories when a table is renamed.As the title says. Applies only to internal (or native) tables.
454	Hive Support escaping of ; in strings in cliIf ; appears in string literals in a query the hive cli is not able to escape them properly.
458	null Setting fs.default.name incorrectly leads to meaningless error messageIn my hadoop-site.xml I accidentally set fs.default.name tohttp://wilbur21.labs.corp.sp1.yahoo.com:8020instead of the proper:hdfs://wilbur21.labs.corp.sp1.yahoo.com:8020The result washive&gt; show tables;FAILED: Unknown exception : nullFAILED: Unknown exception : nullTime taken: 0.035 secondshive&gt;It should give a meaningful error message.
485	ExecReducer ObjectInspector join assumes all columns are stringsjoin assumes all columns are string - pass the objectinspector from execreducer and use that
488	Hive Table Partition loading into a partition with more than one partition column fails if the partition is not created before.Following test fails on HDFS cluster but not on local file system.drop table hive_test_src;drop table hive_test_dst;create table hive_test_src ( col1 string ) stored as textfile ;load data local inpath &amp;apos;../data/files/test.dat&amp;apos; overwrite into table hive_test_src ;create table hive_test_dst ( col1 string ) partitioned by ( pcol1 string , pcol2 string) stored as sequencefile;insert overwrite table hive_test_dst partition ( pcol1=&amp;apos;test_part&amp;apos;, pcol2=&amp;apos;test_part&amp;apos;) select col1 from hive_test_src ;select * from hive_test_dst where pcol1=&amp;apos;test_part&amp;apos; and pcol2=&amp;apos;test_part&amp;apos;;  returns zero rows.drop table hive_test_src;drop table hive_test_dst;
497	Table Select SELECT PredicatePushdown Column predicate pushdown fails if all columns are not selectedpredicate pushdown seems to fail in some scenarios... it is ok if all the columns are selected.create table ppda(a string, b string);select a from ppda where ppda.a &gt; 10; --&gt; failsselect b from ppda where ppda.a &gt; 10; --&gt; okselect * from ppda where ppda.a &gt; 10; --&gt; okselect b from appd where appd.b &gt; 10 and appd.a &gt; 20; --&gt; ok
514	Table Partition partition key names should be case insensitive in alter table add partition statement.create table testpc(a int) partitioned by (ds string, hr string);alter table testpc add partition (ds="1", hr="1"); --&gt; worksalter table testpc add partition (ds="1", Hr="1"); --&gt; doesn&amp;apos;t work
517	null Silent flag does not work on local jobs.The commandshive2 -S -e "from tmp_foo select count(1)" &gt; my_stdout.txtandhive2 -S -hiveconf mapred.job.tracker=local -hiveconf mapred.local.dir=/tmp/foo -e "from tmp_foo select count(1)" &gt; my_stdout.txtgive different results.The former looks like:56and the latter looks like:plan = /tmp/plan61908.xmlNumber of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes):set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers:set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers:set mapred.reduce.tasks=&lt;number&gt;Job running in-process (local Hadoop)map = 100%,  reduce =0%map = 100%,  reduce =100%Ended Job = job_local_156
532	null predicate clause with limit should not be pushed down.in queries like below , &amp;apos;v.c2 &gt; 20&amp;apos; shouldn&amp;apos;t be pushed up.select * from (select * from t where t.c &gt; 20 limit 20) v where v.c2 &gt; 20
534	CliDriver cli adds a new line at the beginning of every querythis results in error messages always specify a line which is one more than the actual error.hive&gt; select count* from abc;FAILED: Parse Error: line 2:14 cannot recognize input &amp;apos;from&amp;apos; in expression specification
543	Hive provide option to run hive in local modethis is a little bit more than just mapred.job.tracker=localwhen run in this mode - multiple jobs are an issue since writing to same tmp directories is an issue. the following options:hadoop.tmp.dirmapred.local.dirneed to be randomized (perhaps based on queryid).
560	MapJoinOperator ColumnPruner SemanticAnalyzer Table SelectOperator column pruning not working with map joinsdrop table tst1;drop table tst2;create table tst1(a1 string, a2 string, a3 string, a4 string);create table tst2(b1 string, b2 string, b3 string, b4 string);explain select /*+ MAPJOIN(a) */ a.a1, a.a2 from tst1 a join tst2 b ON a.a2=b.b2;the select is after the join - column pruning is not happening
578	Optimizer Graph Partition Operator Refactor partition pruning code as an optimizer transformationSome bugs with partition pruning have been reported and the correct fix for many of them is to rewrite the partition pruning code as an optimizer transformation which gets kicked in after the predicate pushdown code. This refactor also uses the graph walker framework so that the partition pruning code gets consolidated well with the frameworks and does not work on the query block but rather works on the operator tree.
592	Table renaming internal table should rename HDFS and also change path of the table and partitions accordingly.rename table changes just the name of the table in metastore but not hdfs. so if a table with old name is created, it uses the hdfs directory pointing to the renamed table.
718	Table Partition Select Date Load data inpath into a new partition without overwrite does not move the fileThe bug can be reproduced as following. Note that it only happens for partitioned tables. The select after the first load returns nothing, while the second returns the data correctly.insert.txt in the current local directory contains 3 lines: "a", "b" and "c".&gt; create table tmp_insert_test (value string) stored as textfile;&gt; load data local inpath &amp;apos;insert.txt&amp;apos; into table tmp_insert_test;&gt; select * from tmp_insert_test;abc&gt; create table tmp_insert_test_p ( value string) partitioned by (ds string) stored as textfile;&gt; load data local inpath &amp;apos;insert.txt&amp;apos; into table tmp_insert_test_p partition (ds = &amp;apos;2009-08-01&amp;apos;);&gt; select * from tmp_insert_test_p where ds= &amp;apos;2009-08-01&amp;apos;;&gt; load data local inpath &amp;apos;insert.txt&amp;apos; into table tmp_insert_test_p partition (ds = &amp;apos;2009-08-01&amp;apos;);&gt; select * from tmp_insert_test_p where ds= &amp;apos;2009-08-01&amp;apos;;a       2009-08-01b       2009-08-01d       2009-08-01
741	JoinOperator HiveInputFormat HiveOutputFormat HiveKey NULL is not handled correctly in joinWith the following data in table input4_cb:Key        Value------       --------NULL     32518          NULLThe following query:select * from input4_cb a join input4_cb b on a.key = b.value;returns the following result:NULL    325    18   NULLThe correct result should be empty set.When &amp;apos;null&amp;apos; is replaced by &amp;apos;&amp;apos; it works.
774	SelectOperator Fix the behavior of "/" and add "DIV"In hive, "select 3/2" will return 1 while MySQL returns 1.5.See http://dev.mysql.com/doc/refman/5.0/en/arithmetic-functions.html#operator_div for details.mysql&gt; select 3/2;+--------+| 3/2    |+--------+| 1.5000 |+--------+1 row in set (0.00 sec)mysql&gt; select 3 div 2;+---------+| 3 div 2 |+---------+|       1 |+---------+1 row in set (0.00 sec)mysql&gt; select -3 div 2;+----------+| -3 div 2 |+----------+|       -1 |+----------+1 row in set (0.00 sec)mysql&gt; select -3 div -2;+-----------+| -3 div -2 |+-----------+|         1 |+-----------+1 row in set (0.00 sec)mysql&gt; select 3 div -2;+----------+| 3 div -2 |+----------+|       -1 |+----------+1 row in set (0.00 sec)
803	null Hive Thrift interface code should ignore fields start with __issetNew versions of Thrift generates a field "_isset_bit_vector" instead of "_isset".We should ignore both cases.
823	null Make table alise in MAPJOIN hint case insensitiveIf we use table alias in upper case for MAPJOIN hint, it is ignored. It must be specified in lower case.Example query:SELECT /*+ MAPJOIN(N) */ parse_url(ADATA.url,&amp;apos;HOST&amp;apos;) AS domain, N.type AS typeFROM nikeusers N join adserves ADATA on (ADATA.user_id = N.uid)WHERE ADATA.data_date = &amp;apos;20090901&amp;apos;This query features reducers in its execution. Attached is output of explain extended.After changing query to:SELECT /*+ MAPJOIN */ parse_url(adata.url,&amp;apos;HOST&amp;apos;) AS domain, n.type AS typeFROM nikeusers n join adserves adata on (adata.user_id = N.uid)WHERE adata.data_date = &amp;apos;20090901&amp;apos;It executes as expected. Attached is output of explain extended.Thanks to Zheng for helping and catching this.
855	UDF UDF: Concat should accept multiple argumentsAccording to mysql, concat should accept multiple arguments.http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_concat
856	GenericUDFConcat allow concat to take more than 2 argumentsmysql&amp;apos;s concat allows concat(&amp;apos;a&amp;apos;, &amp;apos;b&amp;apos;, &amp;apos;c&amp;apos;), but hive&amp;apos;s currently will accept only two arguments.
878	null Update the hash table entry before flushing in Group By hash aggregationThis is a newly introduced bug from r796133.We should first update the aggregation, and then we can flush the hash table. Otherwise the entry that we update might be already out of the hash table.
940	Table Partition restrict creation of partitions with empty partition keyscreate table pc (a int) partitioned by (b string, c string);alter table pc add partition (b="f", c=&amp;apos;&amp;apos;);above alter cmd fails but actually creates a partition with name &amp;apos;b=f/c=&amp;apos; but describe partition on the same name fails. creation of such partitions should not be allowed.
953	null script_broken_pipe3.q brokenThe negative test script_broken_pipe3.q is broken if we allow partial consumption.For now, I have disabled partial consumption. Can you take a look ?
967	Table Implement "show create table"SHOW CREATE TABLE would be very useful in cases where you are trying to figure out the partitioning and/or bucketing scheme for a table. Perhaps this could be implemented by having new tables automatically SET PROPERTIES (create_command=&amp;apos;raw text of the create statement&amp;apos;)?
1039	Hive Query select multi-insert doesn&apos;t work for local directoriesAs wd pointed out in hive-user, the following query only load data to the first local directory. Multi-insert to tables works fine.hive&gt; from test&gt; INSERT OVERWRITE LOCAL DIRECTORY &amp;apos;/home/stefdong/tmp/0&amp;apos; select *where a = 1&gt; INSERT OVERWRITE LOCAL DIRECTORY &amp;apos;/home/stefdong/tmp/1&amp;apos; select *where a = 3;
1046	bug Pass build.dir.hive and other properties to subantCurrently we are not passing properties like "build.dir.hive" etc to subant.We should do that, otherwise setting "build.dir.hive" is not useful.
1056	null Predicate push down does not work with UDTF&apos;sPredicate push down does not work with UDTF&amp;apos;s in lateral viewshive&gt; SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS k WHERE k=1;FAILED: Unknown exception: nullhive&gt;
1116	Table Hive bug with alter table rename when table has property EXTERNAL=FALSEif the location is not an external location - this would be safer.the problem right now is that it&amp;apos;s tricky to use the drop and rename way of writing new data into a table. consider:Initialization block:drop table a_tmpcreate table a_tmp like a;Loading block:load data &lt;newdata&gt; into a_tmp;drop table a;alter table a_tmp rename to a;this looks safe. but it&amp;apos;s not. if one runs this multiple times - then data is lost (since &amp;apos;a&amp;apos; is pointing to &amp;apos;a_tmp&amp;apos;&amp;apos;s location after any iteration. and dropping table &amp;apos;a&amp;apos; blows away loaded data in the next iteration).if the location is being managed by Hive - then &amp;apos;rename&amp;apos; should switch location as well.
1124	Hive Expression CREATE VIEW should expand the query text consistentlyWe should expand the omitted alias in the same way in "select" and in "group by".Hive "Group By" recognize "group by" expressions by comparing the literal string.hive&gt; create view zshao_view as select d, count(1) as cnt from zshao_tt group by d;OKTime taken: 0.286 secondshive&gt; select * from zshao_view;FAILED: Error in semantic analysis: line 1:7 Expression Not In Group By Key d in definition of VIEW zshao_view [select d, count(1) as `cnt` from `zshao_tt` group by `zshao_tt`.`d`] used as zshao_view at line 1:14
1187	Hive Derby MySQL Implement ddldump utility for Hive MetastoreImplement a ddldump utility for the Hive metastore that will generate the QL DDL necessary to recreate the state of the current metastore on another metastore instance.A major use case for this utility is migrating a metastore from one database to another database, e.g. from an embedded Derby instanced to a MySQL instance.The ddldump utility should support the following features:Ability to generate DDL for specific tables or all tables.Ability to specify a table name prefix for the generated DDL, which will be useful for resolving table name conflicts.
1218	null CREATE TABLE t LIKE some_view should create a new empty base table, but instead creates a copy of viewI think it should copy only the column definitions from the view metadata.  Currently it is copying the entire descriptor, resulting in a new view instead of a new base table.
1271	null Case sensitiveness of type information specified when using custom reducer causes type mismatchType information specified  while using a custom reduce script is converted to lower case, and causes type mismatch during query semantic analysis .  The following REDUCE query where field name =  "userId" failed.hive&gt; CREATE TABLE SS (&gt;                     a INT,&gt;                     b INT,&gt;                     vals ARRAY&lt;STRUCT&lt;userId:INT, y:STRING&gt;&gt;&gt;                 );OKhive&gt; FROM (select * from srcTable DISTRIBUTE BY id SORT BY id) s&gt;     INSERT OVERWRITE TABLE SS&gt;     REDUCE *&gt;         USING &amp;apos;myreduce.py&amp;apos;&gt;         AS&gt;                     (a INT,&gt;                     b INT,&gt;                     vals ARRAY&lt;STRUCT&lt;userId:INT, y:STRING&gt;&gt;&gt;                     )&gt;         ;FAILED: Error in semantic analysis: line 2:27 Cannot insert intotarget table because column number/types are different SS: Cannotconvert column 2 from array&lt;struct&lt;userId:int,y:string&gt;&gt; toarray&lt;struct&lt;userid:int,y:string&gt;&gt;.The same query worked fine after changing "userId" to "userid".
1281	null Bucketing column names in create table should be case-insensitiveThis create table fails because &amp;apos;userId&amp;apos; != &amp;apos;userid&amp;apos;CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS;
1315	null bucketed sort merge join breaks after dynamic partition insertbucketed sort merge join produces wrong bucket number due to HIVE-1002 patch, which breaks HIVE-1290.
1341	FilterOperator Filter Operator Column Pruning should preserve the column orderThe column pruning process for the filter operator should preserve the order of input columns, otherwise it could result in miss match in columns in the down stream operators.
1344	Table error in select disinctfrom T a select distinct a.* where a.ds=&amp;apos;2010-05-01&amp;apos;;gets a error
1350	CommandProcessor SessionState hive.query.id is not uniqueif commands are executed by the same user within a second
1378	Hive driver Return value for map, array, and struct needs to return a stringIn order to be able to select/display any data from JDBC Hive driver, return value for map, array, and struct needs to return a string
1385	UDF select UDF field() doesn&apos;t workI tried it against one of my table:hive&gt; desc r;OKkey intvalue stringa stringhive&gt; select * from r;OK4 val_356 NULL4 val_356 NULL484 val_169 NULL484 val_169 NULL2000 val_169 NULL2000 val_169 NULL3000 val_169 NULL3000 val_169 NULL4000 val_125 NULL4000 val_125 NULLhive&gt; select *, field(value, &amp;apos;val_169&amp;apos;) from r;OK4 val_356 NULL 04 val_356 NULL 0484 val_169 NULL 0484 val_169 NULL 02000 val_169 NULL 02000 val_169 NULL 03000 val_169 NULL 03000 val_169 NULL 04000 val_125 NULL 04000 val_125 NULL 0
1418	select column pruning not working with lateral viewselect myCol from tmp_pyang_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;
1460	SELECT Filter Query JOIN should not output rows for NULL valuesWe should filter out rows with NULL keys from the result of this querySELECT * FROM a JOIN b on a.key = b.key
1465	null hive-site.xml ${user.name} not replaced for local-file derby metastore connection URLSeems that for this parameter&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:derby:;databaseName=/var/lib/hive/metastore/${user.name}_db;create=true&lt;/value&gt;&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;${user.name} is never replaced by the actual user name:$ ls -la /var/lib/hive/metastore/total 24drwxrwxrwt 3 root root 4096 Apr 30 12:37 .drwxr-xr-x 3 root root 4096 Apr 30 12:25 ..drwxrwxr-x 5 hadoop hadoop 4096 Apr 30 12:37 ${user.name}_db
1471	Table CTAS should unescape the column name in the select-clause.The following query{{{create table T as select `to` from S;}}}failed since `to` should be unescaped before creating the table.
1494	UDAF Index followup: remove sort by clause and fix a bug in collect_set udaf
1501	null when generating reentrant INSERT for index rebuild, quote identifiers using backticksYongqiang, you mentioned that you weren&amp;apos;t able to do this due to SORT BY not accepting them.  The SORT BY is gone now as of HIVE-1494 (and SORT BY needs to be fixed anyway).
1509	bug Monitor the working set of the number of files
1550	method Implement DROP VIEW IF EXISTSImplement the IF EXISTS clause for the DROP VIEW statement.See http://dev.mysql.com/doc/refman/5.0/en/drop-view.html
1600	Output need to sort hook input/output lists for test result determinismBegin forwarded message:From: Ning Zhang &lt;nzhang@facebook.com&gt;Date: August 26, 2010 2:47:26 PM PDTTo: John Sichi &lt;jsichi@facebook.com&gt;Cc: "hive-dev@hadoop.apache.org" &lt;hive-dev@hadoop.apache.org&gt;Subject: Re: failure in load_dyn_part1.qYes I saw this error before but if it does not repro. So it&amp;apos;s probably an ordering issue in POSTHOOK.On Aug 26, 2010, at 2:39 PM, John Sichi wrote:I&amp;apos;m seeing this failure due to a result diff when running tests on latest trunk:POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=11POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=12-POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=11-POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=12POSTHOOK: Output: default@nzhang_part1@ds=2008-04-08/hr=11POSTHOOK: Output: default@nzhang_part1@ds=2008-04-08/hr=12+POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=11+POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=12Did something change recently?  Or are we missing a Java-level sort on the input/output list for determinism?JVS
1608	null use sequencefile as the default for storing intermediate resultsThe only argument for having a text file for storing intermediate results seems to be better debuggability.But, tailing a sequence file is possible, and it should be more space efficient
1614	null UDTF json_tuple should return null row when input is not a valid JSON stringIf the input column is not a valid JSON string, json_tuple will not return anything but this will prevent the downstream operators to access the left-hand side table. We should output a NULL row instead, similar to when the input column is a NULL value.
1649	TRANSFORM Ability to update counters and status from TRANSFORM scriptsHadoop Streaming supports the ability to update counters and status by writing specially coded messages to the script&amp;apos;s stderr stream.A streaming process can use the stderr to emit counter information. reporter:counter:&lt;group&gt;,&lt;counter&gt;,&lt;amount&gt; should be sent to stderr to update the counter.A streaming process can use the stderr to emit status information. To set a status, reporter:status:&lt;message&gt; should be sent to stderr.Hive should support these same features with its TRANSFORM mechanism.
1654	SemanticAnalyzer select distinct should allow column name regexThis works (matching column name foo):select `fo.*` from pokes;but thisselect distinct `fo.*` from pokes;givesFAILED: Error in semantic analysis: line 1:16 Invalid Table Alias or Column Reference `fo.*`It should work consistently.
1676	Hive HiveCommand ShowTablesDesc ShowTablePropertiesDesc HiveHistory show table extended like does not work well with wildcardsAs evident from the output below though there are tables that match the wildcard, the output from "show table extended like " does not contain the matches.bin/hive -e "show tables &amp;apos;foo*&amp;apos;"Hive history file=/tmp/pradeepk/hive_job_log_pradeepk_201009301037_568707409.txtOKfoofoo2Time taken: 3.417 secondsbin/hive -e "show table extended like &amp;apos;foo*&amp;apos;"Hive history file=/tmp/pradeepk/hive_job_log_pradeepk_201009301037_410056681.txtOKTime taken: 2.948 seconds
1720	bug hbase_stats.q is failingSaw this failure on Hudson and in my own sandbox.https://hudson.apache.org/hudson/job/Hive-trunk-h0.20/392/
1804	MapJoinOperator TableScanOperator Mapjoin will fail if there are no files associating with the join tablesIf there are some empty tables without any file associated, the map join will fail.
1853	null downgrade JDO versionAfter HIVE-1609, we are seeing some table not found errors intermittently.We have a test case where 5 processes are concurrently issueing the same query -explain extended insert .. select from &lt;T&gt;and once in a while, we get a error &lt;T&gt; not found -When we revert back the JDO version, the error is gone.We can investigate later to find the JDO bug, but for now this is a show-stopper for facebook, and needsto be reverted back immediately.This also means, that the filters will not be pushed to mysql.
1856	Signal Implement DROP TABLE/VIEW ... IF EXISTSThis issue combines issues HIVE-1550/1165/1542/1551:augment DROP TABLE/VIEW with IF EXISTSsignal an error if the table/view doesn&amp;apos;t exist and IF EXISTS wasn&amp;apos;t specifiedintroduce a flag in the configuration that allows you to turn off the new behavior
1869	null TestMTQueries failing on jenkinsTestMTQueries has been failing intermittently on Hudson. The first failure I can finda record of on Hudson is from svn rev 1052414 on December 24th, but it&amp;apos;slikely that the failures actually started earlier.
1874	null fix HBase filter pushdown broken by HIVE-1638See comments at end of HIVE-1660 for what happened.
1896	null HBase and Contrib JAR names are missing version numbersAlso, does anyone know why the hbase and contrib JARs use underscoresinstead of dashes in their names? Can I change this or will it break something?./build/dist/lib/hive-anttasks-0.7.0-SNAPSHOT.jar./build/dist/lib/hive-cli-0.7.0-SNAPSHOT.jar./build/dist/lib/hive-common-0.7.0-SNAPSHOT.jar./build/dist/lib/hive-exec-0.7.0-SNAPSHOT.jar./build/dist/lib/hive-hwi-0.7.0-SNAPSHOT.jar./build/dist/lib/hive-jdbc-0.7.0-SNAPSHOT.jar./build/dist/lib/hive-metastore-0.7.0-SNAPSHOT.jar./build/dist/lib/hive-serde-0.7.0-SNAPSHOT.jar./build/dist/lib/hive-service-0.7.0-SNAPSHOT.jar./build/dist/lib/hive-shims-0.7.0-SNAPSHOT.jar./build/dist/lib/hive_contrib.jar                                   &lt;------./build/dist/lib/hive_hbase-handler.jar                     &lt;------
1914	bug failures in testhbaseclidriveri didnt debug it
1975	Hive method method method method "insert overwrite directory" Not able to insert data with multi level directory pathBelow query execution is failedEx:insert overwrite directory &amp;apos;/HIVEFT25686/chinna/&amp;apos; select * from dept_j;
2060	null CLI local mode hit NPE when exiting by ^DCLI gets an NPE when running in local mode and hit an ^D to exit it.
2080	bug Few code improvements in the ql and serde packages.Few code improvements in the ql and serde packages.1) Little performance Improvements2) Null checks to avoid NPEs3) Effective varaible management.
2086	null Add test coverage for external table data loss issueData loss when using "create external table like" statement.1) Set up an external table S, point to location L. Populate data in S.2) Create another external table T, using statement like this:create external table T like S location LMake sure table T point to the same location as the original table S.3) Query table T, see the same set of data in S.4) drop table T.5) Query table S will return nothing, and location L is deleted.
2096	null throw a error if the input is larger than a threshold for index input formatThis can hang for ever.
2101	MapJoinOperator FilterOperator JoinOperator mapjoin sometimes gives wrong results if there is a filter in the on condition"SELECT / * + mapjoin(src1, src2) * / * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key &lt; 10 AND src2.key &gt; 10) JOIN src src3 ON (src2.key = src3.key AND src3.key &lt; 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;" will give wrong results in today&amp;apos;s hive
2178	bug Log related Check style Comments fixesFix Log related Check style Comments
2264	Partition Hive server is SHUTTING DOWN when invalid queries beeing executed.When invalid query is beeing executed, Hive server is shutting down."CREATE TABLE SAMPLETABLE(IP STRING , showtime BIGINT ) partitioned by (ds string,ipz int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;apos;\040&amp;apos;""ALTER TABLE SAMPLETABLE add Partition(ds=&amp;apos;sf&amp;apos;) location &amp;apos;/user/hive/warehouse&amp;apos; Partition(ipz=100) location &amp;apos;/user/hive/warehouse&amp;apos;"
2296	FileSinkOperator bad compressed file names from insert intoWhen INSERT INTO is run on a table with compressed output (hive.exec.compress.output=true) and existing files in the table, it may copy the new files in bad file names:Before INSERT INTO:000000_0.gzAfter INSERT INTO:000000_0.gz000000_0.gz_copy_1This causes corrupted output when doing a SELECT * on the table.Correct behavior should be to pick a valid filename such as:000000_0_copy_1.gz
2334	TABLE DESCRIBE TABLE causes NPE when hive.cli.print.header=true
2344	method method method method method method method method filter filter is removed due to regression of HIVE-1538select * from(select type_bucket,randum123from (SELECT *, cast(rand() as double) AS randum123 FROM tbl where ds = ...) awhere randum123 &lt;=0.1)s where s.randum123&gt;0.1 limit 20;This is returning results...andexplainselect type_bucket,randum123from (SELECT *, cast(rand() as double) AS randum123 FROM tbl where ds = ...) awhere randum123 &lt;=0.1shows that there is no filter.
2398	null Hive server doesn&apos;t return schema for &apos;set&apos; commandThe Hive server does process the CLI commands like &amp;apos;set&amp;apos;, &amp;apos;set -v&amp;apos; sent by ODBC or JDBC clients. But currently only the data is returned to client but not schema for that resultset. This makes it unusable for a ODBC or JDBC client to use this option.
2402	Function select like Function like with empty string is throwing null pointer exceptionselect emp.ename from emp where ename like &amp;apos;&amp;apos;This query is throwing null pointer exception
2455	null Pass correct remoteAddress in proxy user authentication
2465	null Primitive Data Types returning null if the data is out of range of the data type.Primitive Data Types returning null if the input data is out of range of the data type. In this case it is better to log the message with the proper message and actual data then user get to know some data is missing.
2466	null mapjoin_subquery  dump small table (mapjoin table) to the same filein mapjoin_subquery.q  there is a querySELECT /*+ MAPJOIN(z) */ subq.key1, z.valueFROM(SELECT /*+ MAPJOIN */ x.key as key1, x.value as value1, y.key as key2, y.value as value2FROM src1 x JOIN src y ON (x.key = y.key)) subqJOIN srcpart z ON (subq.key1 = z.key and z.ds=&amp;apos;2008-04-08&amp;apos; and z.hr=11);when dump x and z to a local file,there all dump to the same file, so we lost the data of x
2498	GroupByOperator Timestamp Group by operator does not estimate size of Timestamp & Binary data correctlyIt currently defaults to default case and returns constant value, whereas we can do better by getting actual size at runtime.
2517	GroupByOperator StructObjectInspector UnionObjectInspector Support group by on struct typeCurrently group by on struct and union types are not supported. This issue will enable support for those.
2564	Hive Database Set dbname at JDBC URL or propertiesThe current Hive implementation ignores a database name at JDBC URL,though we can set it by executing "use &lt;DBNAME&gt;" statement.I think it is better to also specify a database name at JDBC URL or database properties.Therefore, I&amp;apos;ll attach the patch.
2584	Database Alter table should accept database nameIt would be nice if alter table accepts database name.For example:This would be more useful in certain usecases:alter table DB.Tbl set location &lt;location&gt;;rather than 2 statements.use DB;alter table Tbl set location &lt;location&gt;;
2748	bug Upgrade Hbase and ZK dependciesBoth softwares have moved forward with significant improvements. Lets bump compile time dependency to keep up
2757	null hive can&apos;t find hadoop executor scripts without HADOOP_HOME setThe trouble is that in Hadoop 0.23 HADOOP_HOME has been deprecated. I think it would be really nice if bin/hive can be modified to capture the which hadoopand pass that as a property into the JVM.
2792	null SUBSTR(CAST(<string> AS BINARY)) produces unexpected results
2905	table Desc table can&apos;t show non-ascii commentsWhen desc a table with command line or hive jdbc way, the table&amp;apos;s comment can&amp;apos;t be read.1. I have updated javax.jdo.option.ConnectionURL parameter in hive-site.xml file.jdbc:mysql://...:3306/hive?characterEncoding=UTF-82. In mysql database, the comment field of COLUMNS table can be read normally.
2925	FetchTask SelectOperator LimitOperator FilterOperator Support non-MR fetching for simple queries with select/limit/filter operations onlyIt&amp;apos;s trivial but frequently asked by end-users. Currently, select queries with simple conditions or limit should run MR job which takes some time especially for big tables, making the people irritated.For that kind of simple queries, using fetch task would make them happy.
2955	null Queries consists of metadata-only-query returns always empty valueFor partitioned table, simple query on partition column returns always null or empty value, for example,create table emppart(empno int, ename string) partitioned by (deptno int);.. load partitions..select distinct deptno from emppart; // emptyselect min(deptno), max(deptno) from emppart;  // NULL and NULL
3045	table bug handle method bug column bug column method filter bug column filter bug Partition column values are not valid if any of virtual columns is selectedFor example,hive&gt; select * from srcpart where key &lt; 5;0	val_0	2008-04-08	114	val_4	2008-04-08	110	val_0	2008-04-08	110	val_0	2008-04-08	112	val_2	2008-04-08	110	val_1	2008-04-09	124	val_5	2008-04-09	123	val_4	2008-04-09	122	val_3	2008-04-09	120	val_1	2008-04-09	121	val_2	2008-04-09	12hive&gt; select *, BLOCK__OFFSET__INSIDE__FILE from srcpart where key &lt; 5;0	val_0	2008-04-09	11	9684	val_4	2008-04-09	11	12180	val_0	2008-04-09	11	20880	val_0	2008-04-09	11	26322	val_2	2008-04-09	11	40040	val_1	2008-04-09	11	6824	val_5	2008-04-09	11	11313	val_4	2008-04-09	11	11632	val_3	2008-04-09	11	26290	val_1	2008-04-09	11	43671	val_2	2008-04-09	11	5669
3062	Table QueryResult Database TableName Insert into table overwrites existing table if table name contains uppercase character"Insert into table &lt;table-name&gt; ~~" is expected to append query result into the table. But when the table name contains uppercase character, it overwrite existing table.
3064	Table Select select table table Partition partition in "insert into tablename" statement,if the "tablename" contains uppercase characters this statement will overwrite the tablein "insert into tablename" statement,if the "tablename" contains uppercase characters this statement will overwrite the table.For Example:hive&gt; desc dual;OKdummy   stringTime taken: 1.856 secondshive&gt; select * from dual;OKdummyTime taken: 3.133 secondsdrop table if exists tmp_test_1 ;create EXTERNAL table tmp_test_1 (dummy string) partitioned by (dt string, hr string);insert into table tmp_test_1 partition (dt=&amp;apos;1&amp;apos;, hr=&amp;apos;1&amp;apos;)  select * from dual;insert into table tmp_TEST_1 partition (dt=&amp;apos;1&amp;apos;, hr=&amp;apos;1&amp;apos;) select count from dual;select * from tmp_test_1;Result :OK1       1       1Time taken: 0.121 seconds
3081	null ROFL Moment. Numberator and denaminator typos
3090	Timestamp Table Timestamp type values not having nano-second part breaks rowTimestamp values are reading additional one byte if nano-sec part is zero, breaking following columns.&gt;create table timestamp_1 (t timestamp, key string, value string);&gt;insert overwrite table timestamp_1 select cast(&amp;apos;2011-01-01 01:01:01&amp;apos; as timestamp), key, value from src limit 5;&gt;select t,key,value from timestamp_1;2011-01-01 01:01:01		2382011-01-01 01:01:01		862011-01-01 01:01:01		3112011-01-01 01:01:01		272011-01-01 01:01:01		165&gt;select t,key,value from timestamp_1 distribute by t;2011-01-01 01:01:012011-01-01 01:01:012011-01-01 01:01:012011-01-01 01:01:012011-01-01 01:01:01
3094	Warehouse new partition files and directories should inherit file permissions from parent partition/table dirIn HIVE-2936 changes were made for warehouse table sub directories to inherit the permissions from parent directory. But this applies only to directories created by metastore.When directories (in case of dynamic partitioning) or files are created from the MR job, it uses the defaultBut new partition files or directories created from the MR jobs don&amp;apos;t inherit the permissions.This means that even if the permissions have been granted on table directory for a group, the group will not have permissions on the new partitions.
3099	null add findbugs in build.xmlsee HIVE-1172 and HIVE-2169it was used default rules.exclude filter file is in findbugs\findbugs-exclude.xmlthe result of xml files to html file is not added because no style files.
3126	File Task Generate & build the velocity based Hive tests on windows by fixing the path issues1)Escape the backward slash in Canonical Path if unit test runs on windows.2)Diff comparisona.	Ignore the extra spacing on windowsb.	Ignore the different line endings on windows &amp; Unixc.	Convert the file paths to windows specific. (Handle spaces etc..)3)Set the right file scheme &amp; class path separators while invoking the junit task from
3243	null ignore white space between entries of hive/hbase table mappingIn hive/hbase integration, when creating a hive/hbase table, white space is not ignored in hbase.columns.mapping.e.g. "cf:foo, cf:bar" will create two column families "cf" and " cf" in the underlying hbase table, which is certainly not what the user want and make them confused.
3247	Table Column Hive Metadata select Sorted by order of table not respectedWhen a table a sorted by a column or columns, and data is inserted with hive.enforce.sorting=true, regardless of whether the metadata says the table is sorted in ascending or descending order, the data will be sorted in ascending order.e.g.create table table_desc(key string, value string) clustered by (key) sorted by (key DESC) into 1 BUCKETS;create table table_asc(key string, value string) clustered by (key) sorted by (key ASC) into 1 BUCKETS;insert overwrite table table_desc select key, value from src;insert overwrite table table_asc select key, value from src;select * from table_desc;...96	val_9697	val_9797	val_9798	val_9898	val_98select * from table_asc;...96	val_9697	val_9797	val_9798	val_9898	val_98
3248	method method Qfile method method method lack of semi-colon in .q file leads to missing the next statementset hive.check.par=1select count(1) from src;select count(1) from src;If the above .q file is executed, the first statement is lost.Found this while reviewing https://issues.apache.org/jira/browse/HIVE-2848
3480	null <Resource leak>: Fix the file handle leaks in Symbolic & Symlink related input formats.Noticed these file handle leaks while fixing the Symlink related unit test failures on Windows.
3589	Hive Database Table Command describe/show partition/show tblproperties command should accept database namedescribe command not giving the details when called as describe dbname.tablename.Throwing the error "Table dbname not found".Ex: hive -e "describe masterdb.table1" will throw error"Table masterdb not found"
3628	UDF Provide a way to use counters in Hive through UDFCurrently it is not possible to generate counters through UDF. We should support this.Pig currently allows this.
3682	null when output hive table to file,users should could have a separator of their own choiceBy default,when output hive table to file ,columns of the Hive table are separated by ^A character (that is \001).But indeed users should have the right to set a seperator of their own choice.Usage Example:create table for_test (key string, value string);load data local inpath &amp;apos;./in1.txt&amp;apos; into table for_testselect * from for_test;UT-01default separator is \001 line separator is \ninsert overwrite local directory &amp;apos;./test-01&amp;apos;select * from src ;create table array_table (a array&lt;string&gt;, b array&lt;string&gt;)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &amp;apos;\t&amp;apos;COLLECTION ITEMS TERMINATED BY &amp;apos;,&amp;apos;;load data local inpath "../hive/examples/files/arraytest.txt" overwrite into table table2;CREATE TABLE map_table (foo STRING , bar MAP&lt;STRING, STRING&gt;)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &amp;apos;\t&amp;apos;COLLECTION ITEMS TERMINATED BY &amp;apos;,&amp;apos;MAP KEYS TERMINATED BY &amp;apos;:&amp;apos;STORED AS TEXTFILE;UT-02defined field separator as &amp;apos;:&amp;apos;insert overwrite local directory &amp;apos;./test-02&amp;apos;row format delimitedFIELDS TERMINATED BY &amp;apos;:&amp;apos;select * from src ;UT-03: line separator DO NOT ALLOWED to define as other separatorinsert overwrite local directory &amp;apos;./test-03&amp;apos;row format delimitedFIELDS TERMINATED BY &amp;apos;:&amp;apos;select * from src ;UT-04: define map separatorsinsert overwrite local directory &amp;apos;./test-04&amp;apos;row format delimitedFIELDS TERMINATED BY &amp;apos;\t&amp;apos;COLLECTION ITEMS TERMINATED BY &amp;apos;,&amp;apos;MAP KEYS TERMINATED BY &amp;apos;:&amp;apos;select * from src;
3699	GroupByOperator TableScanOperator SelectOperator SemanticAnalyzer QueryPlan description bug like bug Multiple insert overwrite into multiple tables query stores same results in all tables(Note: This might be related to HIVE-2750)I am doing a query with multiple INSERT OVERWRITE to multiple tables in order to scan the dataset only 1 time, and i end up having all these tables with the same content ! It seems the GROUP BY query that returns results is overwriting all the temp tables.Weird enough, if i had further GROUP BY queries into additional temp tables, grouped by a different field, then all temp tables, even the ones that would have been wrong content are all correctly populated.This is the misbehaving query:FROM nikonINSERT OVERWRITE TABLE e1SELECT qs_cs_s_aid AS Emplacements, COUNT AS ImpressionsWHERE qs_cs_s_cat=&amp;apos;PRINT&amp;apos; GROUP BY qs_cs_s_aidINSERT OVERWRITE TABLE e2SELECT qs_cs_s_aid AS Emplacements, COUNT AS VuesWHERE qs_cs_s_cat=&amp;apos;VIEW&amp;apos; GROUP BY qs_cs_s_aid;It launches only one MR job and here are the results. Why does table &amp;apos;e1&amp;apos; contains results from table &amp;apos;e2&amp;apos; ?! Table &amp;apos;e1&amp;apos; should have been empty (see individual SELECTs further below)hive&gt; SELECT * from e1;OKNULL    21627575 251627576 701690950 221690952 421696705 1991696706 661696730 2291696759 851696893 218Time taken: 0.229 secondshive&gt; SELECT * from e2;OKNULL    21627575 251627576 701690950 221690952 421696705 1991696706 661696730 2291696759 851696893 218Time taken: 0.11 secondsHere is are the result to the indiviual queries (only the second query returns a result set):hive&gt; SELECT qs_cs_s_aid AS Emplacements, COUNT AS Impressions FROM nikonWHERE qs_cs_s_cat=&amp;apos;PRINT&amp;apos; GROUP BY qs_cs_s_aid;(...)OK&lt;- There are no results, this is normalTime taken: 41.471 secondshive&gt; SELECT qs_cs_s_aid AS Emplacements, COUNT AS Vues FROM nikonWHERE qs_cs_s_cat=&amp;apos;VIEW&amp;apos; GROUP BY qs_cs_s_aid;(...)OKNULL  21627575 251627576 701690950 221690952 421696705 1991696706 661696730 2291696759 851696893 218Time taken: 39.607 seconds
3756	operation Table method Warehouse "LOAD DATA" does not honor permission inheritenceWhen a "LOAD DATA" operation is performed the resulting data in hdfs for the table does not maintain permission inheritance. This remains true even with the "hive.warehouse.subdir.inherit.perms" set to true.The issue is easily reproducible by creating a table and loading some data into it. After the load is complete just do a "dfs -ls -R" on the warehouse directory and you will see that the inheritance of permissions worked for the table directory but not for the data.
3853	UDF UDF unix_timestamp is deterministic if an argument is given, but it treated as non-deterministic preventing PPDunix_timestamp is declared as a non-deterministic function. But if user provides an argument, it makes deterministic result and eligible to PPD.
3942	UDF method method method method method method method method method method table method method rows Add UDF month_add and month_subhive (default)&gt; desc function extended month_add;month_add(start_date, num_months) - Returns the date that is num_months after start_date.Synonyms: month_substart_date is a string in the format &amp;apos;yyyy-MM-dd HH:mm:ss&amp;apos; or &amp;apos;yyyy-MM-dd&amp;apos;. num_months is a number. The time part of start_date is ignored.Example:SELECT month_add(&amp;apos;2012-04-12&amp;apos;, 1) FROM src LIMIT 1; --Return 2012-05-12SELECT month_add(&amp;apos;2012-04-12 11:22:31&amp;apos;, 1) FROM src LIMIT 1; --Return 2012-05-12SELECT month_add(cast(&amp;apos;2012-04-12 11:22:31&amp;apos; as timestamp), 1) FROM src LIMIT 1; --Return 2012-05-12hive (default)&gt; desc function extended month_sub;month_sub(start_date, num_months) - Returns the date that is num_months after start_date.Synonyms: month_addstart_date is a string in the format &amp;apos;yyyy-MM-dd HH:mm:ss&amp;apos; or &amp;apos;yyyy-MM-dd&amp;apos;. num_months is a number. The time part of start_date is ignored.Example:SELECT month_sub(&amp;apos;2012-04-12&amp;apos;, 1) FROM src LIMIT 1; --Return 2012-05-12SELECT month_sub(&amp;apos;2012-04-12 11:22:31&amp;apos;, 1) FROM src LIMIT 1; --Return 2012-05-12SELECT month_sub(cast(&amp;apos;2012-04-12 11:22:31&amp;apos; as timestamp), 1) FROM src LIMIT 1; --Return 2012-05-12
4046	Table Partition Column Query Column maskingSometimes data in a table needs to be kept around but made inaccessible. Right now it is possible to offline a table or a partition, but not a specific column of a partition. Also, accessing an offlined table results in an error. With this change, it will be possible to mask a column at the partition level, causing all further queries to that column to return null.
4173	Hive select Filter table table table Hive method select method Hive Ignoring where clause for multitable insertHive is ignoring Filter conditions given at Multi Insert select statement when  Filtering given on Source Query..To highlight this issue, please see below example with where clause (status!=&amp;apos;C&amp;apos;) from employee12 table causing issue and due to which insert filters (batch_id=&amp;apos;12 and batch_id!=&amp;apos;12&amp;apos; )not working and dumping all the data coming from source to both the tables.I have checked the hive execution plan, and didn&amp;apos;t find Filter predicates under for filtering record per insert statementsfrom(from employee12select *where status!=&amp;apos;C&amp;apos;) tinsert into table employee1selectstatus,field1,&amp;apos;T&amp;apos; as field2,&amp;apos;P&amp;apos; as field3,&amp;apos;C&amp;apos; as field4where batch_id=&amp;apos;12&amp;apos;insert into table employee2selectstatus,field1,&amp;apos;D&amp;apos; as field2,&amp;apos;P&amp;apos; as field3,&amp;apos;C&amp;apos; as field4where batch_id!=&amp;apos;12&amp;apos;;It is working fine with single insert. Hive generating plan properly..I am able to reproduce this issue with 8.1 and 9.0 version of Hive.
4239	Compiler Remove lock on compilation stage
4266	bug package Refactor HCatalog code to org.apache.hive.hcatalogCurrently HCatalog code is in packages org.apache.hcatalog.  It needs to now move to org.apache.hive.hcatalog.  Shell classes/interface need to be created for public facing classes so that user&amp;apos;s code does not break.
4274	Table User Table created using HCatalog java client doesn&apos;t set the ownerHCatalog client doesn&amp;apos;t seem to set the owner field. The owner field of the table remains null instead of being populated with the user who created the table. Creating table with hive cli seems to work fine.
4386	Mapper Reducer Table max() and min() return NULL on partition column; distinct() returns nothingpartitioned_table is partitioned on year, month, day.&gt; select max(day) from partitioned_table where year=2013 and month=4;spins up zero mappers, one reducer, and returns NULL.  Same for&gt; select min(day) from ...&gt; select distinct(day) from... returns nothing at all.Using an explicit intermediate table does work:&gt; create table foo_max as select day from partitioned_table where year=2013 and month=4;&gt; select max(day) from foo_max; drop table foo_max;Several map-reduce jobs later, the correct answer is given.
4507	null Fix jdbc to compile under openjdk 7The newer Linux distros are shipping with just openjdk 7. Currently, the jdbc module doesn&amp;apos;t compile because some new methods aren&amp;apos;t implemented.
4868	file When reading an ORC file by an MR job, some Mappers may not be able to process data in some casesLet&amp;apos;s say a stripe of an ORC file is 256 MB and we set the split size for an MR job to 64 MB. Right now, splits are created based on byte ranges.Here is an example:|&lt;-The start of a stripe                |&lt;-The end of a stripev                                       v|---------------------------------------|^                        ^|&lt;- The start of a split |&lt;- The end of a splitSo, for some Mappers, it is possible that there is no start of a stripe within the byte range of a split. Those Mappers will process 0 record. We can improve how splits are created for ORC.
4895	bug Move all HCatalog classes to org.apache.hive.hcatalogmake sure to preserve history in SCM
4911	Hive Server Enable QOP configuration for Hive Server 2 thrift transportThe QoP for hive server 2 should be configurable to enable encryption. A new configuration should be exposed "hive.server2.thrift.sasl.qop". This would give greater control configuring hive server 2 service.
4940	null udaf_percentile_approx.q is not deterministicMakes different result for 20(S) and 23.
4977	null HS2: support an alternate resultset serialization format between client and serverCurrent serialization protocol between client and server as defined in cli_service.thrift results in 2x (or more) throughput degradation compared to HS1.Initial proposal is to introduce HS1 serialization protocol as a negotiable alternative.
5237	GroupByOperator SemanticAnalyzer HiveDriver HiveQueryResultSet Incorrect group-by aggregation in 0.11.0group by with sub queries does not correctly aggregate results in Hive 0.11.0.To reproduce:Put the file1,b2,c2,b3,a3,c4,ain HDFS, and runcreate external table abc (x int, y string) row format delimited fields terminated by &amp;apos;,&amp;apos; location &amp;apos;/data/&amp;apos;;The queryselectx,count(*)from(selectx,yfromabcgroup byx,y) agroup byx;will then give the result2	13	12	14	13	11	1instead of the correct1	12	23	24	1In 0.9.0 and 0.10.0 this is all working correctly.
5328	Date Timestamp Select Date and timestamp type converts invalid strings to &apos;1970-01-01&apos;selectcast(&amp;apos;abcd&amp;apos; as date),cast(&amp;apos;abcd&amp;apos; as timestamp)from src limit 1;This prints out 1970-01-01.
5329	Date Timestamp Select Date and timestamp type converts invalid strings to &apos;1970-01-01&apos;selectcast(&amp;apos;abcd&amp;apos; as date),cast(&amp;apos;abcd&amp;apos; as timestamp)from src limit 1;returns &amp;apos;1970-01-01&amp;apos;
5364	table Partition Query Table Method Method select Method NPE on some queries from partitioned orc tableIf you create a partitioned ORC table with:create table A...PARTITIONED BY (year int,month int,day int)This query will fail:select count from A where where year=2013 and month=9 and day=15;
5419	null Fix schema tool issues with Oracle metastoreAddress oracle schema upgrade script issue in 0.12 and trunk (0.13)
5528	bug hive log file name in local is ".log"In local mode the log is getting written to /tmp/{user.name}/.log instead of /tmp/{user.name}/hive.log
5568	PredicatePushdown count(*) on ORC tables with predicate pushdown on partition columns failIf the query is:select count(*) from orc_table where x = 10;where x is a partition column and predicate pushdown is enabled, you&amp;apos;ll get an array out of bounds exception.
5580	expression push down predicates with an and-operator between non-SARGable predicates will get NPEWhen all of the predicates in an AND-operator in a SARG expression get removed by the SARG builder, evaluation can end up with a NPE. Sub-expressions are typically removed from AND-operators because they aren&amp;apos;t SARGable.
5676	bug Cleanup test cases as done during mavenizationA number of issues where found in HIVE-5107 and we plan on committing them directly to trunk.
5843	null Transaction manager for HiveAs part of the ACID work proposed in HIVE-5317 a transaction manager is required.
5845	table select CTAS failed on vectorized code pathFollowing query fails:create table store_sales_2 stored as orc as select * from alltypesorc;
5901	Query Task Query cancel should stop running MR tasksCurrently, query canceling does not stop running MR job immediately.
5956	Database Table TableName HiveTableName SHOW TBLPROPERTIES doesn&apos;t take db nameIdentified in HIVE-5912.
5972	Hiveserver2 driver driver Hiveserver2 is much slower than hiveserver1we are building ms sql cube by linkedserver connectiong hiveserver with Cloudera&amp;apos;s ODBC driver.There are two test results:1. hiveserver1 running on 2CPUs, 8G mem, took about 8 hours2. hiveserver2 running on 4CPUs, 16 mem, took about 13 hours and 27min (never successful on machine with 2CPUs, 8G mem)Although on both cases, almost all CPUs are busy when building cube.But I cannot understand why hiveserver2 is much slower than hiveserver1, because from doc, hs2 support concurrency, it should be faster than hs1, isn&amp;apos;t it?Thanks.CDH4.3 on CentOS6.
6049	null Hive uses deprecated hadoop configuration in Hadoop 2.0Running hive CLI on hadoop 2.0, you&amp;apos;ll see deprecated configurations warnings like this:13/12/14 01:00:51 INFO Configuration.deprecation: mapred.input.dir.recursive isdeprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive13/12/14 01:00:52 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.rackis deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.nodeis deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
6159	null Hive uses deprecated hadoop configuration in Hadoop 2.0Build hive against hadoop 2.0. Then run hive CLI, you&amp;apos;ll see deprecated configurations warnings like this:13/12/14 01:00:51 INFO Configuration.deprecation: mapred.input.dir.recursive isdeprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive13/12/14 01:00:52 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.rackis deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.nodeis deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
6176	Beeline Beeline gives bogus error message if an unaccepted command line option is given$ beeline -o-o (No such file or directory)Beeline version 0.13.0-SNAPSHOT by Apache Hivebeeline&gt;The message suggests that beeline accepts a file (without -f option) while it enters interactive mode any way.
6209	null &apos;LOAD DATA INPATH ... OVERWRITE ..&apos; doesn&apos;t overwrite current dataIn case where user loads data into table using overwrite, using a different file, it is not being overwritten.$ hdfs dfs -cat /tmp/dataaaabbbccc$ hdfs dfs -cat /tmp/data2dddeeefff$ hivehive&gt; create table test (id string);hive&gt; load data inpath &amp;apos;/tmp/data&amp;apos; overwrite into table test;hive&gt; select * from test;aaabbbccchive&gt; load data inpath &amp;apos;/tmp/data2&amp;apos; overwrite into table test;hive&gt; select * from test;aaabbbcccdddeeefffIt seems it is broken by HIVE-3756 which added another condition to whether "rmr" should be run on old directory, and skips in this case.There is a workaround of set fs.hdfs.impl.disable.cache=true;which sabotages this condition, but this condition should be removed in long-term.
6218	TABLE STATISTICS Stats for row-count not getting updated with Tez insert + dbclass=counterInserting data into hive with Tez,  the stats on row-count is not getting updated when using the counter dbclass.To reproduce, run "ANALYZE TABLE store_sales COMPUTE STATISTICS;" with tez as the execution engine.
6295	MetadataOnlyOptimizer metadata_only test on Tez generates unoptimized planOne of the queries in the test should be a 0 stage metadata only query, but on tez it still produces a table scan.
6410	Table Schema Allow output serializations separators to be set for HDFS path as well.HIVE-3682 adds functionality for users to set serialization constants for &amp;apos;insert overwrite local directory&amp;apos;. The same functionality should be available for hdfs path as well. The workaround suggested is to create a table with required format and insert into the table, which enforces the users to know the schema of the result and create the table ahead. Though that works, it is good to have the functionality for loading into directory as well.I&amp;apos;m planning to add the same functionality in &amp;apos;insert overwrite directory&amp;apos; in this jira.
6420	Derby upgrade script for Hive 13 is missing for DerbyThere&amp;apos;s an upgrade script for all DSes but not for Derby. Nothing needs to be done in that script but I&amp;apos;m being told that some tools might break if there&amp;apos;s no matching file.
6469	HiveMetastore Warehouse skipTrash option in hive command lineTh current behavior of hive metastore during a "drop table &lt;table_name&gt;" command is to delete the data from HDFS warehouse and put it into Trash.Currently there is no way to provide a flag to tell the warehouse to skip trash while deleting table data.This ticket is to add skipTrash configuration "hive.warehouse.data.skipTrash" , which when set to true, will skipTrash while dropping table data from hdfs warehouse. This will be set to false by default to keep current behavior.This would be good feature to add, so that an admin of the cluster can specify when not to put data into the trash directory (eg. in a dev environment) and thus not to fill hdfs space instead of relying on trash interval and policy configuration to take care of disk filling issue.
6492	null limit partition number involved in a table scanTo protect the cluster, a new configure variable "hive.limit.query.max.table.partition" is added to hive configuration tolimit the table partitions involved in a table scan.The default value will be set to -1 which means there is no limit by default.This variable will not affect "metadata only" query.
6506	null hcatalog should automatically work with new tableproperties in ORCHIVE-5504 has changes to handle existing table properties for ORC file format. But it does not automatically pick newly added table properties. We should refactor ORC so that its table property list can be automatically determined.
6542	bug build error with jdk 7
6561	Beeline Hive Hive Beeline should accept -i option to Initializing a SQL fileHive CLI has -i option. From Hive CLI help:...-i &lt;filename&gt;                    Initialization SQL file...However, Beeline has no such option:xzhang@xzlt:~/apa/hive3$ ./packaging/target/apache-hive-0.14.0-SNAPSHOT-bin/apache-hive-0.14.0-SNAPSHOT-bin/bin/beeline -u jdbc:hive2:// -i hive.rc...Connected to: Apache Hive (version 0.14.0-SNAPSHOT)Driver: Hive JDBC (version 0.14.0-SNAPSHOT)Transaction isolation: TRANSACTION_REPEATABLE_READ-i (No such file or directory)Property "url" is requiredBeeline version 0.14.0-SNAPSHOT by Apache Hive...
6693	Select CASE with INT and BIGINT failCREATE TABLE testCase (n BIGINT)select case when (n &gt;3) then n else 0 end from testCasefail with error :[Error 10016]: Line 1:36 Argument type mismatch &amp;apos;0&amp;apos;: The expression after ELSE should have the same type as those after THEN: "bigint" is expected but "int" is found&amp;apos;.bigint and int should be more compatible, at least int should implictly cast to bigint.
6833	Hive Query method Output Writer method Row method method when output hive table query to HDFS file,users should have a separator of their own choiceHIVE-3682 allows user to store output of Hive query to a local file along with delimiters and separators of their choice.e.g. insert overwrite local directory &amp;apos;/users/home/XYZ&amp;apos; row format delimited FIELDS TERMINATED BY &amp;apos;,&amp;apos; SELECT * from table_name;Storing query output with default separator(\001) to HDFS is possible.e.g. insert overwrite directory &amp;apos;/user/xYZ/security&amp;apos; SELECT * from table_name;But user can not store output of Hive query to a HDFS directory with delimiters and separators of their choice.e.g. insert overwrite directory &amp;apos;/user/xYZ/security&amp;apos;  row format delimited FIELDS TERMINATED BY &amp;apos;,&amp;apos; SELECT * from table_name; (Gives ERROR)
6847	null Improve / fix bugs in Hive scratch dir setupCurrently, the hive server creates scratch directory and changes permission to 777 however, this is not great with respect to security. We need to create user specific scratch directories instead. Also refer to HIVE-6782 1st iteration of the patch for approach.
6933	TableName "alter table add partition" should support db_name.table_name as table name.Currently,"alter table table_name add partition ...." works.but"alter table db_name.table_name add partition ..." throws an error message (different error for different versions).For consistency, I suggest with support both ways of referring to a table.
6935	Hive Warehouse skipTrash option in hive command linehive drop table command deletes the data from HDFS warehouse and puts it into Trash.Currently there is no way to provide flag to tell warehouse to skip trash while deleting table data.This ticket is to add skipTrash feature in hive command-line, that looks as following.hive -e "drop table skipTrash testTable"This would be good feature to add, so that user can specify when not to put data into trash directory and thus not to fill hdfs space instead of relying on trash interval and policy configuration to take care of disk filling issue.
7129	null Change datanucleus.fixedDatastore config to trueMuch safer in production environment to have this as true.
7164	Hive partition Support non-string partition types in HCatalogCurrently querying hive tables with non-string partition columns using HCat  gives us the following error.Error: Filtering is supported only on partition keys of type stringRelated discussion here : https://www.mail-archive.com/dev@hive.apache.org/msg18011.html
7200	Beeline Beeline output displays column heading even if --showHeader=false is setA few minor/cosmetic issues with the beeline CLI.1) Tool prints the column headers despite setting the --showHeader to false. This property only seems to affect the subsequent header information that gets printed based on the value of property "headerInterval" (default value is 100).2) When "showHeader" is true &amp; "headerInterval &gt; 0", the header after the first interval gets printed after &lt;headerInterval - 1&gt; rows. The code seems to count the initial header as a row, if you will.3) The table footer(the line that closes the table) does not get printed if the "showHeader" is false. I think the table should get closed irrespective of whether it prints the header or not.0: jdbc:hive2://localhost:10000&gt; select * from stringvals;+------+| val  |+------+| t    || f    || T    || F    || 0    || 1    |+------+6 rows selected (3.998 seconds)0: jdbc:hive2://localhost:10000&gt; !set headerInterval 20: jdbc:hive2://localhost:10000&gt; select * from stringvals;+------+| val  |+------+| t    |+------+| val  |+------+| f    || T    |+------+| val  |+------+| F    || 0    |+------+| val  |+------+| 1    |+------+6 rows selected (0.691 seconds)0: jdbc:hive2://localhost:10000&gt; !set showHeader false0: jdbc:hive2://localhost:10000&gt; select * from stringvals;+------+| val  |+------+| t    || f    || T    || F    || 0    || 1    |6 rows selected (1.728 seconds)
7212	null Use resource re-localization instead of restarting sessions in Tezscriptfile1.q is failing on Tez because of a recent breakage in localization. On top of that we&amp;apos;re currently restarting sessions if the resources have changed. (add file/add jar/etc). Instead of doing this we should just have tez relocalize these new resources. This way no session/AM restart is required.
7316	Hive Hive fails on zero length filesFlume will, at times, generate zero length files. This causes queries to fail on Avro data and likely sequence file as well.
7379	Beeline Beeline to fetch full stack trace for job (task) failuresWhen a query submitted via Beeline fails, Beeline displays a generic error message as below:FAILED: Execution Error, return code 1 fromThis is expected, as Beeline is basically a regular JDBC client and is hence limited by JDBC&amp;apos;s capabilities today. But it would be useful if Beeline can return the full remote stack trace and task diagnostics or job ID.
7434	Beeline HiveCLI OutputFormat beeline should not always enclose the output by default in CSV/TSV modeWhen using beeline in CSV/TSV mode (via command !outputformat csv) , the output is always enclosed in single quotes. This is however not the case for Hive CLI, so we need to make this enclose optional.
7475	Beeline Hive Beeline requires newline at the end of each query in a fileWhen using the -f option on beeline its required to have a newline at the end of each query otherwise the connection is closed before the query is run.$ cat test.hqlshow databases;%$ beeline -u jdbc:hive2://localhost:10000 --incremental=true -f test.hqlscan complete in 3msConnecting to jdbc:hive2://localhost:10000Connected to: Apache Hive (version 0.13.1)Driver: Hive JDBC (version 0.13.1)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 0.13.1 by Apache Hive0: jdbc:hive2://localhost:10000&gt; show databases;Closing: 0: jdbc:hive2://localhost:10000
7566	null HIVE can&apos;t count hbase NULL column value properlyHBase table structure is like this:table name : &amp;apos;testtable&amp;apos;column family : &amp;apos;data&amp;apos;column 1 : &amp;apos;name&amp;apos;column 2 : &amp;apos;color&amp;apos;HIVE mapping table is structure is like this:table name : &amp;apos;hb_testtable&amp;apos;column 1 : &amp;apos;name&amp;apos;column 2 : &amp;apos;color&amp;apos;in hbase, put two rowsJames, blueMaythen do select in hiveselect * from hb_testtable where color is nullthe result isMay, NULLthen try countselect count from hb_testtable where color is nullthe result is 0, which should be 1
7681	AlterTableSetPropertiesOperation AlterTableChangeColumnOperation AlterTableAddPartitionOperation AlterTableDropPartitionOperation HiveMetaStoreClient qualified tablenames usage does not work with several alter-table commandsChanges were made in HIVE-4064 for use of qualified table names in more types of queries. But several alter table commands don&amp;apos;t work with qualifiedalter table default.tmpfoo set tblproperties ("bar" = "bar value")ALTER TABLE default.kv_rename_test CHANGE a a STRINGadd,drop partitionalter index rebuild
7944	Partition Table Column current update stats for columns of a partition of a table is not correctWe worked hard towards faster update stats for columns of a partition of a table previouslyhttps://issues.apache.org/jira/browse/HIVE-7736andhttps://issues.apache.org/jira/browse/HIVE-7876Although there is some improvement, it is only correct in the first run. There will be duplicate column stats later. Thanks to Eugene Koifman &amp;apos;s comments
7982	bug method Hive MySQL database query Regression in explain with CBO enabled due to issuing query per K,V for the statsNow explain for Q17 is back in the 12 second range, I checked the queries issues to MySQL and they are very different than beforeon August 15 explain was completing in under 5 seconds and we issued the following queries :select "COLUMN_NAME", "COLUMN_TYPE", min("LONG_LOW_VALUE"), max("LONG_HIGH_VALUE"), min("DOUBLE_LOW_VALUE"), max("DOUBLE_HIGH_VALUE"), min("BIG_DECIMAL_LOW_VALUE"), max("BIG_DECIMAL_HIGH_VALUE"), sum("NUM_NULLS"), max("NUM_DISTINCTS"), max("AVG_COL_LEN"), max("MAX_COL_LEN"), sum("NUM_TRUES"), sum("NUM_FALSES") from "PART_COL_STATS" where "DB_NAME" = &amp;apos;tpcds_bin_partitioned_orc_30000&amp;apos; and "TABLE_NAME" = &amp;apos;store_returns&amp;apos; and "COLUMN_NAME" in (&amp;apos;sr_item_sk&amp;apos;,&amp;apos;sr_customer_sk&amp;apos;,&amp;apos;sr_ticket_number&amp;apos;) AND "PARTITION_NAME" in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=1998-01-07&amp;apos;,..&amp;apos;sr_returned_date=2003-07-01&amp;apos;) group by "COLUMN_NAME", "COLUMN_TYPE";select "COLUMN_NAME", "COLUMN_TYPE", min("LONG_LOW_VALUE"), max("LONG_HIGH_VALUE"), min("DOUBLE_LOW_VALUE"), max("DOUBLE_HIGH_VALUE"), min("BIG_DECIMAL_LOW_VALUE"), max("BIG_DECIMAL_HIGH_VALUE"), sum("NUM_NULLS"), max("NUM_DISTINCTS"), max("AVG_COL_LEN"), max("MAX_COL_LEN"), sum("NUM_TRUES"), sum("NUM_FALSES") from "PART_COL_STATS" where "DB_NAME" = &amp;apos;tpcds_bin_partitioned_orc_30000&amp;apos; and "TABLE_NAME" = &amp;apos;store_returns&amp;apos; and "COLUMN_NAME" in (&amp;apos;sr_returned_date_sk&amp;apos;) AND "PARTITION_NAME" in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;..&amp;apos;sr_returned_date=2003-07-01&amp;apos;) group by "COLUMN_NAME", "COLUMN_TYPE"Currently explain Q17 takes 11 seconds and the queries sent to MySQL are very inefficient because1) They no longer do the aggregation on MySQL and get a row per partition2) There is a query per stats K,V pair so the number of queries is up by 9xselect COLUMN_NAME, COLUMN_TYPE, count(PARTITION_NAME)  from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos;  and COLUMN_NAME in (&amp;apos;sr_item_sk&amp;apos;,&amp;apos;sr_customer_sk&amp;apos;,&amp;apos;sr_ticket_number&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) group by COLUMN_NAME, COLUMN_TYPEselect COLUMN_NAME, sum(NUM_NULLS), sum(NUM_TRUES), sum(NUM_FALSES) from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos;  and COLUMN_NAME in (&amp;apos;sr_customer_sk&amp;apos;,&amp;apos;sr_item_sk&amp;apos;,&amp;apos;sr_ticket_number&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) group by COLUMN_NAMEselect LONG_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_customer_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;LONG_LOW_VALUE&amp;apos;select LONG_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_customer_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;LONG_HIGH_VALUE&amp;apos;select DOUBLE_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_customer_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;DOUBLE_LOW_VALUE&amp;apos;select DOUBLE_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_customer_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;DOUBLE_HIGH_VALUE&amp;apos;select BIG_DECIMAL_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_customer_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;BIG_DECIMAL_LOW_VALUE&amp;apos;select BIG_DECIMAL_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_customer_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;BIG_DECIMAL_HIGH_VALUE&amp;apos;select NUM_DISTINCTS,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_customer_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;NUM_DISTINCTS&amp;apos;select AVG_COL_LEN,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_customer_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;AVG_COL_LEN&amp;apos;select MAX_COL_LEN,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_customer_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;MAX_COL_LEN&amp;apos;select LONG_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_item_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;LONG_LOW_VALUE&amp;apos;select LONG_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;apos;tpcds_bin_partitioned_orc_200&amp;apos; and TABLE_NAME = &amp;apos;store_returns&amp;apos; and COLUMN_NAME in (&amp;apos;sr_item_sk&amp;apos;) and PARTITION_NAME in (&amp;apos;sr_returned_date=1998-01-06&amp;apos;,&amp;apos;sr_returned_date=2003-07-01&amp;apos;) order by &amp;apos;LONG_HIGH_VALUE&amp;apos;
8231	Table table table select table Error when insert into empty table with ACIDSteps to show the bug :1. create tablecreate table encaissement_1b_64m like encaissement_1b;2. check tabledesc encaissement_1b_64m;dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m;everything is ok:0: jdbc:hive2://nc-h04:10000/casino&gt; desc encaissement_1b_64m;                                                                                                              +------------+------------+----------+--+|  col_name  | data_type  | comment  |+------------+------------+----------+--+| id         | int        |          || idmagasin  | int        |          || zibzin     | string     |          || cheque     | int        |          || montant    | double     |          || date       | timestamp  |          || col_6      | string     |          || col_7      | string     |          || col_8      | string     |          |+------------+------------+----------+--+9 rows selected (0.158 seconds)0: jdbc:hive2://nc-h04:10000/casino&gt; dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/;+-------------+--+| DFS Output  |+-------------+--++-------------+--+No rows selected (0.01 seconds)3. Insert values into the new tableinsert into table encaissement_1b_64m VALUES (1, 1, &amp;apos;800000000909000000000000&amp;apos;, 1, 12.5, &amp;apos;12/05/2014&amp;apos;, &amp;apos;&amp;apos;,&amp;apos;&amp;apos;,&amp;apos;&amp;apos;);4. Check0: jdbc:hive2://nc-h04:10000/casino&gt; select id from encaissement_1b_64m;+-----+--+| id  |+-----+--++-----+--+No rows selected (0.091 seconds)There are already a pb. I don&amp;apos;t see the inserted row.5. When I&amp;apos;m checking HDFS directory, I see delta_0000421_0000421 folder0: jdbc:hive2://nc-h04:10000/casino&gt; dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/;+-----------------------------------------------------------------------------------------------------------------------------------------------------+--+|                                                                     DFS Output                                                                      |+-----------------------------------------------------------------------------------------------------------------------------------------------------+--+| Found 1 items                                                                                                                                       || drwxr-xr-x   - hduser supergroup          0 2014-09-23 12:17 hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/delta_0000421_0000421  |+-----------------------------------------------------------------------------------------------------------------------------------------------------+--+2 rows selected (0.014 seconds)6. Doing a major compaction solves the bug0: jdbc:hive2://nc-h04:10000/casino&gt; alter table encaissement_1b_64m compact &amp;apos;major&amp;apos;;No rows affected (0.046 seconds)0: jdbc:hive2://nc-h04:10000/casino&gt; dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/;+--------------------------------------------------------------------------------------------------------------------------------------------+--+|                                                                 DFS Output                                                                 |+--------------------------------------------------------------------------------------------------------------------------------------------+--+| Found 1 items                                                                                                                              || drwxr-xr-x   - hduser supergroup          0 2014-09-23 12:21 hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/base_0000421  |+--------------------------------------------------------------------------------------------------------------------------------------------+--+2 rows selected (0.02 seconds)
8298	SemanticAnalyzer HiveSemanticAnalyzerHookContext HiveSemanticAnalyzerHook Incorrect results for n-way join when join expressions are not in same order across joinsselect *  from srcpart a join srcpart b on a.key = b.key and a.hr = b.hr join srcpart c on a.hr = c.hr and a.key = c.key;is minimal query which reproduces it
8461	QueryResult DecimalUtil Make Vectorized Decimal query results match Non-Vectorized query results with respect to trailing zeroes... .0000
8541	Decimal Decimal values contains extra trailing zeros when vectorization is onThe fix done on HIVE-7373 preserves the trailing zeroes from decimal valuesas they are read from the table files, but when vectorization is on, thesevalues contain extra tralinig zeroes up to what the scale allows when aggregationexpressions are used.Here&amp;apos;s an example (data gotten from vector_decimal_aggregate.q):hive&gt; SET hive.vectorized.execution.enabled=false;hive&gt; SELECT cint, MAX(cdecimal2) max FROM decimal_vgby GROUP BY cint HAVING COUNT(*) &gt; 1;+------------+---------------------+--+|    cint    |         max         |+------------+---------------------+--+| NULL       | 11160.715384615385  || -3728      | 6984454.211097692   || -563       | -617.5607769230769  || 762        | 6984454.211097692   || 6981       | 6984454.211097692   || 253665376  | 11697.969230769231  || 528534767  | 6984454.211097692   || 626923679  | 11645.746153846154  |+------------+---------------------+--+hive&gt; SET hive.vectorized.execution.enabled=true;hive&gt; SELECT cint, MAX(cdecimal2) max FROM decimal_vgby GROUP BY cint HAVING COUNT(*) &gt; 1;+------------+-------------------------+--+|    cint    |          max2           |+------------+-------------------------+--+| NULL       | 11160.71538461538500    || -3728      | 6984454.21109769200000  || -563       | -617.56077692307690     || 762        | 6984454.21109769200000  || 6981       | 6984454.211097692       || 253665376  | 11697.96923076923100    || 528534767  | 6984454.21109769200000  || 626923679  | 11645.74615384615400    |+------------+-------------------------+--+Hive should not add trailing zeroes when aggregation functions are used.
8604	null Re-enable auto_sortmerge_join_5 on tez
8614	bug Upgrade hive to use tez version 0.5.2-SNAPSHOT
8653	Filter CBO: Push Semi Join through, Project/Filter/JoinCLEAR LIBRARY CACHE
8690	bug entity method Move Avro dependency to 1.7.7Move Avro dependency from 1.7.5 to current release 1.7.7.
8768	null CBO: Fix filter selectivity for "in clause" & "<>"
9194	SemanticAnalyzer ASTNode HiveConf HiveException SelectOperator ColumnInfo RowResolver OperatorFactory TableScanOperator Support select distinct *As per Laljo John Pullokkaran&amp;apos;s review comments, implement select distinct *
9278	TABLE SELECT Cached expression feature broken in one caseDifferent query result depending on whether hive.cache.expr.evaluation is true or false.  When true, no query results are produced (this is wrong).The q file:set hive.cache.expr.evaluation=true;CREATE TABLE cache_expr_repro (date_str STRING);LOAD DATA LOCAL INPATH &amp;apos;../../data/files/cache_expr_repro.txt&amp;apos; INTO TABLE cache_expr_repro;SELECT MONTH(date_str) AS `mon`, CAST((MONTH(date_str) - 1) / 3 + 1 AS int) AS `quarter`,   YEAR(date_str) AS `year` FROM cache_expr_repro WHERE ((CAST((MONTH(date_str) - 1) / 3 + 1 AS int) = 1) AND (YEAR(date_str) = 2015)) GROUP BY MONTH(date_str), CAST((MONTH(date_str) - 1) / 3 + 1 AS int),   YEAR(date_str) ;cache_expr_repro.txt2015-01-01 00:00:002015-02-01 00:00:002015-01-01 00:00:002015-02-01 00:00:002015-01-01 00:00:002015-01-01 00:00:002015-02-01 00:00:002015-02-01 00:00:002015-01-01 00:00:002015-01-01 00:00:00
9357	UDF Create ADD_MONTHS UDFADD_MONTHS adds a number of months to startdate:add_months(&amp;apos;2015-01-14&amp;apos;, 1) = &amp;apos;2015-02-14&amp;apos;add_months(&amp;apos;2015-01-31&amp;apos;, 1) = &amp;apos;2015-02-28&amp;apos;add_months(&amp;apos;2015-02-28&amp;apos;, 2) = &amp;apos;2015-04-30&amp;apos;add_months(&amp;apos;2015-02-28&amp;apos;, 12) = &amp;apos;2016-02-29&amp;apos;
9459	bug method Hive description Concat plus date functions appear to be broken in 0.14In the below example I create year_month and month_year vars. These each should be yyyymm and mmyyyy integer strings but it appears as if hive is calling the first function twice such that it is returning mmmm and yyyyyyyy.hive&gt; select&gt; month(a.joined) month,&gt; year(a.joined) year,&gt; concat(cast(year(a.joined) as string),cast(month(a.joined) as string)) year_month,&gt; concat(cast(month(a.joined) as string),cast(year(a.joined) as string)) month_year&gt; from a limit 20;OKmonth	year	year_month	month_year7	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	777	2014	20142014	77Time taken: 0.109 seconds, Fetched: 20 row(s)Other users appear to experience similar issues in this stack overflow: http://stackoverflow.com/questions/27740866/convert-date-to-decimal-format-in-hive .I tested this in 0.13 and 0.14 and it does not appear to be an issue in 0.13.I looked around and could not find a similar issue so hopefully this is not a duplicate.
9481	null allow column list specification in INSERT statementGiven a table FOO(a int, b int, c int), ANSI SQL supports insert into FOO(c,b) select x,y from T.  The expectation is that &amp;apos;x&amp;apos; is written to column &amp;apos;c&amp;apos; and &amp;apos;y&amp;apos; is written column &amp;apos;b&amp;apos; and &amp;apos;a&amp;apos; is set to NULL, assuming column &amp;apos;a&amp;apos; is NULLABLE.Hive does not support this.  In Hive one has to ensure that the data producing statement has a schema that matches target table schema.Since Hive doesn&amp;apos;t support DEFAULT value for columns in CREATE TABLE, when target schema is explicitly provided, missing columns will be set to NULL if they are NULLABLE, otherwise an error will be raised.If/when DEFAULT clause is supported, this can be enhanced to set default value rather than NULL.Thus, givencreate table source (a int, b int);create table target (x int, y int, z int);create table target2 (x int, y int, z int);insert into target(y,z) select * from source;will meaninsert into target select null as x, a, b from source;andinsert into target(z,y) select * from source;will meantinsert into target select null as x, b, a from source;Also,from sourceinsert into target(y,z) select null as x, *insert into target2(y,z) select null as x, source.*;and for partitioned tables, givenGiven:CREATE TABLE pageviews (userid VARCHAR(64), link STRING, "from" STRING)PARTITIONED BY (datestamp STRING) CLUSTERED BY (userid) INTO 256 BUCKETS STORED AS ORC;INSERT INTO TABLE pageviews PARTITION (datestamp = &amp;apos;2014-09-23&amp;apos;)(userid,link)VALUES (&amp;apos;jsmith&amp;apos;, &amp;apos;mail.com&amp;apos;);And dynamic partitioningINSERT INTO TABLE pageviews PARTITION (datestamp)(userid,datestamp,link)VALUES (&amp;apos;jsmith&amp;apos;, &amp;apos;2014-09-23&amp;apos;, &amp;apos;mail.com&amp;apos;);In all cases, the schema specification contains columns of the target table which are matched by position to the values produced by VALUES clause/SELECT statement.  If the producer side provides values for a dynamic partition column, the column should be in the specified schema.  Static partition values are part of the partition spec and thus are not produced by the producer and should not be part of the schema specification.
9486	null Use session classloader instead of application loaderFrom http://www.mail-archive.com/dev@hive.apache.org/msg107615.htmlLooks reasonable
9495	TABLE SELECT Map Side aggregation affecting map performanceWhen trying to run a simple aggregation query with hive.map.aggr=true, map tasks take a lot of time in Hive 0.14 as against  with hive.map.aggr=false.e.g.Consider the query:INSERT OVERWRITE TABLE lineitem_tgt_aggselect alias.a0 as a0,alias.a2 as a1,alias.a1 as a2,alias.a3 as a3,alias.a4 as a4from (select alias.a0 as a0,SUM(alias.a1) as a1,SUM(alias.a2) as a2,SUM(alias.a3) as a3,SUM(alias.a4) as a4from (select lineitem_sf500.l_orderkey as a0,CAST(lineitem_sf500.l_quantity * lineitem_sf500.l_extendedprice * (1 - lineitem_sf500.l_discount) * (1 + lineitem_sf500.l_tax) as double) as a1,lineitem_sf500.l_quantity as a2,CAST(lineitem_sf500.l_quantity * lineitem_sf500.l_extendedprice * lineitem_sf500.l_discount as double) as a3,CAST(lineitem_sf500.l_quantity * lineitem_sf500.l_extendedprice * lineitem_sf500.l_tax as double) as a4from lineitem_sf500) aliasgroup by alias.a0) alias;The above query was run with ~376GB of data / ~3billion records in the source.It takes ~10 minutes with hive.map.aggr=false.With map side aggregation set to true, the map tasks don&amp;apos;t complete even after an hour.
9546	Beeline Hive Create table taking substantially longer time when other select queries are run in parallel.Create table taking substantially longer time when other select queries are run in parallel.We were able to reproduce the issue using beeline in two sessions.Beeline Shell 1:a) create table with no other queries running on hive ( took approximately 0.313 seconds)b) Insert Data into the tablec) Run a select count query on the above tableBeeline Shell 2:a) create table while step c) is running in the Beeline Shell 1. (took approximately 60.431 seconds)
9616	Select Table Column Hive 0.14Hi,I am using hive 0.14 version which will support all crud operation as said by support teamI am not able to select specific columns to insert, likeinsert into table table1 id,name,sal select id,name,sal from table2 where table1.id = table2.id
9632	bug method Hive description Date Date query partition inconsistent results between year(), month(), day(), and the actual values in formulasIn wanting to create a date dimension value which would match our existing database environment, I figured I would be able to do as I have done in the past and use the following formula:(year(date)*10000)+(month(date)*100)+day(date)Given the date of 2015-01-09, the above formula should result in a value of 20150109.  Instead, the resulting value is 20353515.SELECT&gt; adjusted_activity_date_utc,&gt; year(adjusted_activity_date_utc),&gt; month(adjusted_activity_date_utc),&gt; day(adjusted_activity_date_utc),&gt; (year(adjusted_activity_date_utc)*10000)+(month(adjusted_activity_date_utc)*100)+day(adjusted_activity_date_utc),&gt; (year(adjusted_activity_date_utc)*10000),&gt; (month(adjusted_activity_date_utc)*100),&gt; day(adjusted_activity_date_utc)&gt; from event_histories limit 5;OKadjusted_activity_date_utc	_c1	_c2	_c3	_c4	_c5	_c6	_c72015-01-09	2015	1	9	20353515	20150000	100	92015-01-09	2015	1	9	20353515	20150000	100	92015-01-09	2015	1	9	20353515	20150000	100	92015-01-09	2015	1	9	20353515	20150000	100	92015-01-09	2015	1	9	20353515	20150000	100	9Oddly enough, this works as expected when a specific date value is used for the column.I have tried this with partition and non-partition columns and found the result to be the same.SELECT&gt; adjusted_activity_date_utc,&gt; year(adjusted_activity_date_utc),&gt; month(adjusted_activity_date_utc),&gt; day(adjusted_activity_date_utc),&gt; (year(adjusted_activity_date_utc)*10000)+(month(adjusted_activity_date_utc)*100)+day(adjusted_activity_date_utc),&gt; (year(adjusted_activity_date_utc)*10000),&gt; (month(adjusted_activity_date_utc)*100),&gt; day(adjusted_activity_date_utc)&gt; from event_histories&gt; where adjusted_activity_date_utc = &amp;apos;2015-01-09&amp;apos;&gt; limit 5;OKadjusted_activity_date_utc	_c1	_c2	_c3	_c4	_c5	_c6	_c72015-01-09	2015	1	9	20150109	20150000	100	92015-01-09	2015	1	9	20150109	20150000	100	92015-01-09	2015	1	9	20150109	20150000	100	92015-01-09	2015	1	9	20150109	20150000	100	92015-01-09	2015	1	9	20150109	20150000	100	9
9669	bug selected columnsHi Team,In Hive 1.0, selected columns patch is updated?Because i am not able to insert selected columns.I am using Hive 1.0 bin, how can i apply patch directly in Hive instead of trunk or source.Thanks in advance.
10026	null LLAP: AM should get notifications on daemons going down or restartingThere&amp;apos;s lost state otherwise, which can cause queries to hang.
10048	driver JDBC - Support SSL encryption regardless of Authentication mechanismJDBC driver currently only supports SSL Transport if the Authentication mechanism is SASL Plain with username and password. SSL transport  should be decoupled from Authentication mechanism. If the customer chooses to do Kerberos Authentication and SSL encryption over the wire it should be supported. The Server side already supports this but the driver does not.
10493	null Merge multiple joins when join keys are the sameCBO return path: auto_join3.q is joined on the same key from 3 sources. It is translated into 2 map joins. Need to merge them into a single one.
10541	Beeline Beeline requires newline at the end of each query in a fileBeeline requires newline at the end of each query in a file.
10709	null Update Avro version to 1.7.7We should update the avro version to 1.7.7 to consumer some of the nicer compatibility features.
10755	bug Rework on HIVE-5193 to enhance the column oriented table accessAdd the support of column pruning for column oriented table access which was done in HIVE-5193 but was reverted due to the join issue in HIVE-10720.In 1.3.0, the patch posted by Viray didn&amp;apos;t work, probably due to some jar reference. That seems to get fixed and that patch works in 2.0.0 now.
10965	HiveSQLException direct SQL for stats fails in 0-column case
10996	null Aggregation / Projection over Multi-Join Inner Query producing incorrect resultsWe see the following problem on 1.1.0 and 1.2.0 but not 0.13 which seems like a regression.The following query (Q1) produces no results:select sfrom (select last.*, action.st2, action.nfrom (select purchase.s, purchase.timestamp, max (mevt.timestamp) as last_stage_timestampfrom (select * from purchase_history) purchasejoin (select * from cart_history) mevton purchase.s = mevt.swhere purchase.timestamp &gt; mevt.timestampgroup by purchase.s, purchase.timestamp) lastjoin (select * from events) actionon last.s = action.s and last.last_stage_timestamp = action.timestamp) list;While this one (Q2) does produce results :select *from (select last.*, action.st2, action.nfrom (select purchase.s, purchase.timestamp, max (mevt.timestamp) as last_stage_timestampfrom (select * from purchase_history) purchasejoin (select * from cart_history) mevton purchase.s = mevt.swhere purchase.timestamp &gt; mevt.timestampgroup by purchase.s, purchase.timestamp) lastjoin (select * from events) actionon last.s = action.s and last.last_stage_timestamp = action.timestamp) list;1	21	20	Bob	12341	31	30	Bob	12343	51	50	Jeff	1234The setup to test this is:create table purchase_history (s string, product string, price double, timestamp int);insert into purchase_history values (&amp;apos;1&amp;apos;, &amp;apos;Belt&amp;apos;, 20.00, 21);insert into purchase_history values (&amp;apos;1&amp;apos;, &amp;apos;Socks&amp;apos;, 3.50, 31);insert into purchase_history values (&amp;apos;3&amp;apos;, &amp;apos;Belt&amp;apos;, 20.00, 51);insert into purchase_history values (&amp;apos;4&amp;apos;, &amp;apos;Shirt&amp;apos;, 15.50, 59);create table cart_history (s string, cart_id int, timestamp int);insert into cart_history values (&amp;apos;1&amp;apos;, 1, 10);insert into cart_history values (&amp;apos;1&amp;apos;, 2, 20);insert into cart_history values (&amp;apos;1&amp;apos;, 3, 30);insert into cart_history values (&amp;apos;1&amp;apos;, 4, 40);insert into cart_history values (&amp;apos;3&amp;apos;, 5, 50);insert into cart_history values (&amp;apos;4&amp;apos;, 6, 60);create table events (s string, st2 string, n int, timestamp int);insert into events values (&amp;apos;1&amp;apos;, &amp;apos;Bob&amp;apos;, 1234, 20);insert into events values (&amp;apos;1&amp;apos;, &amp;apos;Bob&amp;apos;, 1234, 30);insert into events values (&amp;apos;1&amp;apos;, &amp;apos;Bob&amp;apos;, 1234, 25);insert into events values (&amp;apos;2&amp;apos;, &amp;apos;Sam&amp;apos;, 1234, 30);insert into events values (&amp;apos;3&amp;apos;, &amp;apos;Jeff&amp;apos;, 1234, 50);insert into events values (&amp;apos;4&amp;apos;, &amp;apos;Ted&amp;apos;, 1234, 60);I realize select * and select s are not all that interesting in this context but what lead us to this issue was select count(distinct s) was not returning results. The above queries are the simplified queries that produce the issue.I will note that if I convert the inner join to a table and select from that the issue does not appear.Update: Found that turning off  hive.optimize.remove.identity.project fixes this issue. This optimization was introduced in https://issues.apache.org/jira/browse/HIVE-8435
11034	method method method method method Joining multiple tables producing different results with different order of joinJoin between tables with different join columns from main table yielding wrong results in hive.Changing the order of the joins between main table and other tables is producing different results.Please see below for the steps to reproduce the issue:1. Create tables as follows:create table p(ck string, email string);create table a1(ck string, flag string);create table a2(email string, flag string);create table a3(ck string, flag string);2. Load data into the tables as follows:Pckemail10e1020e2030e3040e40A1ckflag10N20Y30Y40YA2emailflage10Ye20Ne30Ye40YA3ckflag10Y20Y30N40Y3. Good query:select p.ckfrom pleft outer join a1 on p.ck = a1.ckleft outer join a3 on p.ck = a3.ckleft outer join a2 on p.email = a2.emailwhere a1.flag = &amp;apos;Y&amp;apos;and a3.flag = &amp;apos;Y&amp;apos;and a2.flag = &amp;apos;Y&amp;apos;;and results are404. Bad queryselect p.ckfrom pleft outer join a1 on p.ck = a1.ckleft outer join a2 on p.email = a2.emailleft outer join a3 on p.ck = a3.ckwhere a1.flag = &amp;apos;Y&amp;apos;and a2.flag = &amp;apos;Y&amp;apos;and a3.flag = &amp;apos;Y&amp;apos;;Producing results as:3040
11049	bug With Clause should cache data & reuseHive supports with clause. However Hive don&amp;apos;t cache the result set of with clause and reuse it.Instead we inline the query definition of the with clause. This results in re execution of with clause query every where it is referenced.
11424	Rule Rule to transform OR clauses into IN clauses in CBOWe create a rule that will transform OR clauses into IN clauses (when possible).
11502	GroupByOperator HiveConf MapRedTask MapWork HiveAggregate HiveUtils HiveCatalog Map side aggregation is extremely slowFor the query as following:create table tbl2 asselect col1, max(col2) as col2from tbl1 group by col1;If the column for group by has many different values (for example 400000) and it is in type double, the map side aggregation is very slow. I ran the query which took more than 3 hours , after 3 hours, I have to kill the query.The same query can finish in 7 seconds, if I turn off map side aggregation by:set hive.map.aggr = false;
11566	null Hybrid grace hash join should only allocate write buffer for a hash partition when first write happensCurrently it&amp;apos;s allocating one write buffer for a number of hash partitions up front, which can cause GC pause.It&amp;apos;s better to do the write buffer allocation on demand.
11607	null Export tables broken for data > 32 MBBroken for both hadoop-1 as well as hadoop-2 line
11752	QB Pre-materializing complex CTE queriesCurrently, hive regards CTE clauses as a simple alias to the query block, which makes redundant works if it&amp;apos;s used multiple times in a query. This introduces a reference threshold for pre-materializing the CTE clause as a volatile table (which is not exists in any form of metastore and just accessible from QB).
11798	Beeline The Beeline report should not display the header when --showHeader is set to false.In Beeline tool User sets the --showheader option as false.In command line interface user inputs the command bin/beeline -u jdbc:hive2://10.19.92.183:10000  --showHeader=falseActual Result : The Beeline report displays the column name.Expected Result : The Beeline report should not display the header when --showHeader is set to false.
11863	TestCliDriver FS based stats collection generates wrong results for tez (for union queries)FS based stats collection is the default way to collect stats. However, there are some cases (involving unions) where it generates wrong results. Refer test case in HIVE-11860 and compare test cli driver results against tez results. Also it will be good to extend statsfs.q test case with union queries.
11918	Hive optimizer optimizer Implement/Enable constant related optimization rules in CalciteRight now, Hive optimizer (Calcite) is short of the constant related optimization rules. For example, constant folding, constant propagation and constant transitive rules. Although Hive later provides those rules in the logical optimizer, we would like to implement those inside Calcite. This will benefit the current optimization as well as the optimization based on return path that we are planning to use in the future. This JIRA is the umbrella JIRA to implement/enable those rules.
11931	null Join sql cannot get resultI found a join issue in hive-1.2.1 and hive-1.1.1.The create table sql is as below.CREATE TABLE IF NOT EXISTS join_case(orderid  bigint,tradeitemid bigint,id bigint) ROW FORMAT DELIMITEDFIELDS TERMINATED BY &amp;apos;,&amp;apos;LINES TERMINATED BY &amp;apos;\n&amp;apos;STORED AS TEXTFILE;Please put attached sample data file 000000_0 in /tmp/join_case folder.Then load data.LOAD DATA LOCAL INPATH &amp;apos;/tmp/join_case/000000_0&amp;apos; OVERWRITE INTO TABLE join_case;Run the following sql, but cannot get searching result.select a.id from(select orderid as orderid, max(id) as id from join_case group by orderid) ajoin(select id as id , orderid as orderid from join_case) bon a.id = b.id limit 10;This issue also occurs in hive-1.1.0-cdh5.4.5.But in apache hive-1.0.1 the above sql can return 10 rows.After exchanging the sequence of "orderid as orderid" and "max(id) as id", the following sql can get result in hive-1.2.1 and hive-1.1.1.select a.id from(select max(id) as id, orderid as orderid from join_case group by orderid) ajoin(select id as id , orderid as orderid from join_case) bon a.id = b.id limit 10;Also, the following sql can get results in hive-1.2.1 and hive-1.1.1.select a.id from(select orderid as orderid, id as id from join_case group by orderid, id) ajoin(select id as id , orderid as orderid from join_case) bon a.id = b.id limit 10;Anyone can take a look at this issue?Thanks.
12017	Query Do not disable CBO by default when number of joins in a query is equal or less than 1Instead, we could disable some parts of CBO that are not relevant if the query contains 1 or 0 joins. Implementation should be able to define easily other query patterns for which we might disable some parts of CBO (in case we want to do it in the future).
12040	null CBO: Use CBO, even for the 1 JOIN + GROUP BY caseHive CBO kicks in for a query only if a query has &gt;1 joins, which is an archaic result of only re-ordering joins in the original impl.As more group-by &amp; filter optimizers have been added, modify the CBO impl to kick in even if the query has a single JOIN.
12249	null Improve logging with tezWe need to improve logging across the board. TEZ-2851 added a caller context so that one can correlate logs with the application. This jira adds a new configuration for users that can be used to correlate the logs.
12261	schema hive schema schematool version info exit status should depend on compatibility, not equalityNewer versions of metastore schema are compatible with older versions of hive, as only new tables or columns are added with additional information.HIVE-11613 added a check in hive schematool -info command to see if schema version is equal.However, the state where db schema version is ahead of hive software version is often seen when a &amp;apos;rolling upgrade&amp;apos; or &amp;apos;rolling downgrade&amp;apos; is happening. This is a state where hive is functional and returning non zero status for it is misleading.
12266	Hive Beeline Command When client exists abnormally, it doesn&apos;t release ACID locksif you start Hive CLI (locking enabled) and run some command that acquires locks and ^C the shell before command completes the locks for the command remain until they timeout.I believe Beeline has the same issue.Need to add proper hooks to release locks when command dies. (As much as possible)
12275	Query method query method method Query results on macro_duplicate.q are in different order on some environmentsOn my Linux VM, the order is different from the golden file. Seems to work on Mac as well as on the Pre-Commit tests. We can add an order-by to make the results deterministic across environments.55d54&lt; 16    25      24      120     8       10      656a56&gt; 16    25      24      120     8       10      6Exception: Client Execution results failed with error code = 1
12277	null Hive macro results on macro_duplicate.q different after adding ORDER BYAdded an order-by to the query in macro_duplicate.q:-select math_square(a), math_square(b),factorial(a), factorial(b), math_add(a), math_add(b),int(c) from macro_testing;\ No newline at end of file+select math_square(a), math_square(b),factorial(a), factorial(b), math_add(a), math_add(b),int(c) from macro_testing order by int(c);And the results from math_add() changed unexpectedly:-1      4       1       2       2       4       3-16     25      24      120     8       10      6+1      4       1       2       1       4       3+16     25      24      120     16      25      6
12304	Database FunctionRegistry "drop database cascade" needs to unregister functionsCurrently "drop database cascade" command doesn&amp;apos;t unregister the functions under the database. If the functions are not unregistered, in some cases like "describe db1.func1" will still show the info for the function; or if the same database is recreated, "drop if exists db1.func1" will throw an exception since the function is considered existing from the registry while it doesn&amp;apos;t exist in metastore.
12330	bug Fix precommit Spark test part2Regression because of HIVE-11489
12363	Timestamp driver Incorrect results with orc ppd across ORC versionsRun vector_decimal_cast.q on tez cli driver.The issue is related to the ORC Timestamp column stats, which does not exist in all ORC files.When the timestamp column is missing stats, default to YES_NO_NULL instead of assuming the column is all nulls.
12367	Database Lock/unlock database should add current database to inputs and outputs of authz hook
12384	Operator Hive Operation Decimal Method Order Method Select Method Method Method Union Operator may produce incorrect result on TEZUnion queries may produce incorrect result on TEZ.TEZ removes union op, thus might loose the implicit cast in union op.Reproduction test case:set hive.cbo.enable=false;set hive.execution.engine=tez;select (x/sum over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select &amp;apos;100000000&amp;apos; x from (select * from src limit 2) s3)u order by y;select (x/sum over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u order by y;
12432	Hive TABLE Hive on Spark Counter "RECORDS_OUT" always  be zeroA simple way to reproduce :set hive.execution.engine=spark;CREATE TABLE  test(id INT);insert into test values (1) ,(2);
12435	Hive SELECT table table SELECT COUNT(CASE WHEN...) GROUPBY returns 1 for &apos;NULL&apos; in a case of ORC and vectorization is enabled.Run the following query:create table count_case_groupby (key string, bool boolean) STORED AS orc;insert into table count_case_groupby values (&amp;apos;key1&amp;apos;, true),(&amp;apos;key2&amp;apos;, false),(&amp;apos;key3&amp;apos;, NULL),(&amp;apos;key4&amp;apos;, false),(&amp;apos;key5&amp;apos;,NULL);The table contains the following:key1	truekey2	falsekey3	NULLkey4	falsekey5	NULLThe below query returns:SELECT key, COUNT(CASE WHEN bool THEN 1 WHEN NOT bool THEN 0 ELSE NULL END) AS cnt_bool0_ok FROM count_case_groupby GROUP BY key;key1	1key2	1key3	1key4	1key5	1while it expects the following results:key1	1key2	1key3	0key4	1key5	0The query works with hive ver 1.2. Also it works when a table is not orc format.Also even if it&amp;apos;s an orc table, when vectorization is disabled, the query works.
12437	RecordReader SMB join in tez fails when one of the tables is emptyIt looks like a better check for empty tables is to depend on the existence of the record reader for the input from tez.
12445	null Tracking of completed dags is a slow memory leakLLAP daemons track completed DAGs, but never clean up these structures. This is primarily to disallow out of order executions. Evaluate whether that can be avoided - otherwise this structure needs to be cleaned up with a delay.
12479	Date Vectorization: Vectorized Date UDFs with up-stream JoinsThe row-counts expected with and without vectorization differ.The attached small-scale repro case produces 5 rows with vectorized multi-key joins and 53 rows without the vectorized join.
12489	Hive TABLE STATISTICS Analyze for partition fails if partition value has special charactersWhen analyzing a partition that has a special characters in the value, the analyze command fails with an exception.Example:hive&gt; create table testtable (a int) partitioned by (b string);hive&gt; insert into table testtable  partition (b="p\"1") values (1);hive&gt; ANALYZE TABLE testtable  PARTITION(b="p\"1") COMPUTE STATISTICS for columns a;
12503	null GBY-Join transpose rule may go in infinite loopThis happens when pushing aggregate is not found to be any cheaper. Can be reproduced by running cbo_rp_auto_join1.q with flag turned on.
12566	method method method SELECT method TABLE method ROW method Incorrect result returns when using COALESCE in WHERE condition with LEFT JOINThe left join query with on/where clause returns incorrect result (more rows are returned). See the reproducible sample below.Left table with data:CREATE TABLE ltable (i int, la int, lk1 string, lk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;apos;,&amp;apos;;---1,\N,CD5415192314304,000712,\N,CD5415192225530,00071Right  table with data:CREATE TABLE rtable (ra int, rk1 string, rk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;apos;,&amp;apos;;---1,CD5415192314304,0007145,CD5415192314304,00072Query:SELECT * FROM ltable l LEFT OUTER JOIN rtable r on (l.lk1 = r.rk1 AND l.lk2 = r.rk2) WHERE COALESCE(l.la,&amp;apos;EMPTY&amp;apos;)=COALESCE(r.ra,&amp;apos;EMPTY&amp;apos;);Result returns:1	NULL	CD5415192314304	00071	NULL	NULL	NULL2	NULL	CD5415192225530	00071	NULL	NULL	NULLThe correct result should be2	NULL	CD5415192225530	00071	NULL	NULL	NULL
12568	driver Provide an option to specify network interface used by Spark remote client [Spark Branch]Spark client sends a pair of host name and port number to the remote driver so that the driver can connects back to HS2 where the user session is. Spark client has its own way determining the host name, and pick one network interface if the host happens to have multiple network interfaces. This can be problematic. For that, there is parameter, hive.spark.client.server.address, which user can pick an interface. Unfortunately, this interface isn&amp;apos;t exposed.Instead of exposing this parameter, we can use the same logic as Hive in determining the host name. Therefore, the remote driver connecting to HS2 using the same network interface as a HS2 client would do.There might be a case where user may want the remote driver to use a different network. This is rare if at all. Thus, for now it should be sufficient to use the same network interface.
12574	TABLE select partition order rows windowing function returns incorrect result when the window size is larger than the partition sizeIn PTF windowing, when the partition is small and the window size is larger than the partition size, we are seeing incorrect result. It happens for max, min, first_value, last_value and sum functions.CREATE TABLE sdy1(ord int,type string);The data is:2 a3 a1 aThe result is as follows for the query select ord, min(ord) over (partition by type order by ord rows between 1 preceding and 7 following)1 12 13 1The expected result is:1 12 13 2
12575	null Similar to HIVE-12574, collect_set and count() give incorrect result when partition size is smaller than window sizeWill  fix these two functions separately since seems the issue and fix would be different from the other functions like max() and last_value(), etc.
12603	Query Partition Add config to block queries that scan > N number of partitionsStrict mode is useful for blocking queries that load all partitions, but it&amp;apos;s still possible to put significant load on the HMS for queries that scan a large number of partitions. It would be useful to add a config provide a hard limit to the number of partitions scanned by a query.
12609	null Remove javaXML serializationWe use kryo as default serializer and javaXML based serialization is not used in many places and is also not well tested. We should remove javaXML serialization and make kryo as the only serialization option.
12610	null Hybrid Grace Hash Join should fail task faster if processing first batch fails, instead of continuing processing the restDuring processing the spilled partitions, if there&amp;apos;s any fatal error, such as Kryo exception, then we should exit early, instead of moving on to process the rest of spilled partitions.
12635	Hive Timestamp Hive Column Timestamp Hive should return the latest hbase cell timestamp as the row timestamp valueWhen hive talks to hbase and maps hbase timestamp field to one hive column,  seems hive returns the first cell timestamp instead of the latest one as the timestamp value.Makes sense to return the latest timestamp since adding the latest cell can be  considered an update to the row.
12656	null Turn hive.compute.query.using.stats on by defaultWe now have hive.compute.query.using.stats=false by default. We plan to turn it on by default so that we can have better performance. We can also set it to false in some test cases to maintain the original purpose of those tests..
12687	null LLAP Workdirs need to default to YARN localLLAP_DAEMON_WORK_DIRS("hive.llap.daemon.work.dirs", ""is a bad default &amp; fails at startup if not overridden.A better default would be to fall back onto YARN local dirs if this is not configured.
12728	Schema Table Serde File Apply DDL restrictions for ORC schema evolutionHIVE-11981 added schema evolution for ORC. However, it does not enforce any restrictions in DDL that can break schema evolution. Following changes have to be enforced in DDL to support the assumptions in schema evolution (that columns will only be added).1) Restrict changing the file format of the table2) Restrict changing the serde of the table3) Restrict replacing columns to not drop columns or do unsupported type widening4) Restrict reordering columns5) Restrict unsupported type promotions
12742	SELECT NULL table comparison within CASE does not work as previous hive versionsdrop table test_1;create table test_1 (id int, id2 int);insert into table test_1 values (123, NULL);SELECT cast(CASE WHEN id = id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS bFROM test_1;--NULLBut the output should be true (confirmed with postgres.)
12749	ConstantPropagateProcFactory HiveConf Constant propagate returns string values in incorrect formatSTEP 1. Create and upload test dataExecute in command line:nano stest.dataAdd to file:000126,000777000126,000778000126,000779000474,000888000468,000889000272,000880hadoop fs -put stest.data /hive&gt; create table stest(x STRING, y STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;apos;,&amp;apos;;hive&gt; LOAD DATA  INPATH &amp;apos;/stest.data&amp;apos; OVERWRITE INTO TABLE stest;STEP 2. Execute test query (with cast for x)select x from stest where cast(x as int) = 126;EXPECTED RESULT:000126000126000126ACTUAL RESULT:126126126STEP 3. Execute test query (no cast for x)hive&gt; select x from stest where  x = 126;EXPECTED RESULT:000126000126000126ACTUAL RESULT:126126126In steps #2, #3 I expected &amp;apos;000126&amp;apos; because the origin type of x is STRING in stest table.Note, setting hive.optimize.constant.propagation=false fixes the issue.hive&gt; set hive.optimize.constant.propagation=false;hive&gt; select x from stest where  x = 126;OK000126000126000126Related to HIVE-11104, HIVE-8555
12768	BinarySortableSerDe Thread safety: binary sortable serde decimal deserializationWe see thread safety issues due to static decimal buffer in binary sortable serde.
12786	null CBO may fail for recoverable errorsIn some cases, CBO may generate an error from which it may be possible to recover.
12788	HiveConf UnionProcessor SemanticAnalyzer Hive HiveDriver HiveDatabaseMetaData HiveStatement HiveQueryResultSet HivePreparedStatement HiveConnection Setting hive.optimize.union.remove to TRUE will break UNION ALL with aggregate functionsSee the test case below:0: jdbc:hive2://localhost:10000/default&gt; create table test (a int);0: jdbc:hive2://localhost:10000/default&gt; insert overwrite table test values (1);0: jdbc:hive2://localhost:10000/default&gt; set hive.optimize.union.remove=true;No rows affected (0.01 seconds)0: jdbc:hive2://localhost:10000/default&gt; set hive.mapred.supports.subdirectories=true;No rows affected (0.007 seconds)0: jdbc:hive2://localhost:10000/default&gt; SELECT COUNT(1) FROM test UNION ALL SELECT COUNT(1) FROM test;+----------+--+| _u1._c0  |+----------+--++----------+--+UNION ALL without COUNT function will work as expected:0: jdbc:hive2://localhost:10000/default&gt; select * from test UNION ALL SELECT * FROM test;+--------+--+| _u1.a  |+--------+--+| 1      || 1      |+--------+--+Run the same query without setting hive.mapred.supports.subdirectories and hive.optimize.union.remove to true will give correct result:0: jdbc:hive2://localhost:10000/default&gt; set hive.optimize.union.remove;+-----------------------------------+--+|                set                |+-----------------------------------+--+| hive.optimize.union.remove=false  |+-----------------------------------+--+0: jdbc:hive2://localhost:10000/default&gt; SELECT COUNT(1) FROM test UNION ALL SELECT COUNT(1) FROM test;+----------+--+| _u1._c0  |+----------+--+| 1        || 1        |+----------+--+
12792	OperationType HIVE-12075 didn&apos;t update operation type for plugins
12807	bug Thrift and DB Changes for HIVE-12352This ticket just covers the thrift and DB changes necessary for HIVE-12352
12808	Table query query optimizer table function Filter bug description Logical PPD: Push filter clauses through PTF(Windowing) into TSSimplified repro case of HCC #8880, with the slow query showing the push-down miss.And the manually rewritten query to indicate the expected one.Part of the problem could be the window range not being split apart for PPD, but the FIL is not pushed down even if the rownum filter is removed.create temporary table positions (regionid string, id bigint, deviceid string, ts string);insert into positions values(&amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;apos;, 1422792010, &amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;apos;, &amp;apos;2016-01-01&amp;apos;),(&amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;apos;, 1422792010, &amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;apos;, &amp;apos;2016-01-01&amp;apos;),(&amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;apos;, 1422792010, &amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;apos;, &amp;apos;2016-01-02&amp;apos;),(&amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;apos;, 1422792010, &amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;apos;, &amp;apos;2016-01-02&amp;apos;);-- slow queryexplainWITH t1 AS(SELECT   *,Row_number() over ( PARTITION BY regionid, id, deviceid ORDER BY ts DESC) AS rownosFROM     positions ),latestposition as (SELECT *FROM   t1WHERE  rownos = 1)SELECT *FROM   latestpositionWHERE  regionid=&amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;apos;AND    id=1422792010AND    deviceid=&amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;apos;;-- fast queryexplainWITH t1 AS(SELECT   *,Row_number() over ( PARTITION BY regionid, id, deviceid ORDER BY ts DESC) AS rownosFROM     positionsWHERE  regionid=&amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;apos;AND    id=1422792010AND    deviceid=&amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;apos;),latestposition as (SELECT *FROM   t1WHERE  rownos = 1)SELECT *FROM   latestposition;
12809	null Vectorization: fast-path for coalesce if input.noNulls = trueCoalesce can skip processing other columns, if all the input columns are non-null.Possibly retaining, isRepeating=true.
12814	schema Make thrift and DB changes for HIVE-11444This JIRA tracks the Thrift and DB schema changes for HIVE-11444.  It depends on HIVE-12807.
12816	Schema Thrift and schema changes for HIVE-11685This JIRA depends on HIVE-12814.
12818	bug Schema changes for HIVE-12353This JIRA just covers RDBMS schema changes for HIVE-12353.
12819	null Thrift and RDBMS schema changes for HIVE-11957
12820	null Remove the check if carriage return and new line are used for separator or escape characterThe change in HIVE-11785 doesn&amp;apos;t allow \r or \n to be used as separator or escape character which may break some existing tables which uses \r as separator or escape character e.g..This case actually can be supported regardless of SERIALIZATION_ESCAPE_CRLF set or not.
12821	null Thrift and RDBMS schema changes for HIVE-11965
12822	null Thrift and RDBMS schema changes for HIVE-11495
12823	null Thrift and RDBMS schema changes for HIVE-11956
12824	Function Function CBO doesnt get triggered when aggregate function is used within windowing function
12829	null Thrift changes for HIVE-12686
12830	bug Thrift changes for HIVE-11793
12831	null Thrift and RDBMS changes for HIVE-10249
12832	bug RDBMS schema changes for HIVE-11388
12867	bug Semantic Exception Error Msg should be with in the range of "10000 to 19999"At many places errors encountered during semantic exception is translated as generic error(GENERIC_ERROR, 40000) msg as opposed to semantic error msg.
12893	Partition Sorted dynamic partition does not work if subset of partition columns are constant foldedIf all partition columns are constant folded then sorted dynamic partitioning should not be used as it is similar to static partitioning. But if only subset of partition columns are constant folded sorted dynamic partition optimization will be helpful. Currently, this optimization is disabled if atleast one partition column constant folded.
12905	MapJoinOperator TezProcessor JoinOperator Issue with mapjoin in tez under certain conditionsIn a specific case where we have an outer join followed by another join on the same key and the non-outer side of the outer join is empty, hive-on-tez produces incorrect results.
12911	null PPD might get exercised even when flag is false if CBO is onIntroduced in HIVE-11865.
12931	ShuffleHandler Shuffle tokens stay around forever in LLAPShuffle tokens are never cleaned up, resulting in a slow leak.
12933	Beeline Beeline will hang when authenticating with PAM when libjpam.so is missingWhen we setup PAM authentication, we need to have libjpam.so under java.library.path. If it happens to misplace the .so file, rather than giving an exception, the client will hang forever.Seems we should catch the exception when the lib is missing.
12935	null LLAP: Replace Yarn registry with Zookeeper registryExisting YARN registry service for cluster membership has to depend on refresh intervals to get the list of instances/daemons that are running in the cluster. Better approach would be replace it with zookeeper based registry service so that custom listeners can be added to update healthiness of daemons in the cluster.
12941	null Unexpected result when using MIN() on struct with NULL in first fieldUsing MIN() on struct with NULL in first field of a row yields NULL as result.Example:select min(a) FROM (select 1 as a union all select 2 as a union all select cast(null as int) as a) tmp;OK_c01As expected. But if we wrap it in a struct:select min(a) FROM (select named_struct("field",1) as a union all select named_struct("field",2) as a union all select named_struct("field",cast(null as int)) as a) tmp;OK_c0NULLUsing MAX() works as expected for structs.
12963	SemanticAnalyzer ReduceSinkOperator Table HiveConf LIMIT statement with SORT BY creates additional MR job with hardcoded only one reducerI execute query:hive&gt; select age from test1 sort by age.age  limit 10;Total jobs = 2Launching Job 1 out of 2Number of reduce tasks not specified. Estimated from input data size: 1Launching Job 2 out of 2Number of reduce tasks determined at compile time: 1When I have a large number of rows then the last stage of the job takes a long time. I think we could allow to user choose number of reducers of last job or refuse extra MR job.The same behavior I observed with querie:hive&gt; create table new_test as select age from test1 group by age.age  limit 10;
12993	Beeline user and password supplied from URL is overwritten by the empty user and password of the JDBC connection string when it&apos;s calling from beelineWhen we make the call beeline -u "jdbc:hive2://localhost:10000/;user=aaa;password=bbb", the user and password are overwritten by the blank ones since internally it constructs a "connect &lt;url&gt; &amp;apos;&amp;apos; &amp;apos;&amp;apos; &lt;driver&gt;" call with empty user and password.
12995	null LLAP: Synthetic file ids need collision checksLLAP synthetic file ids do not have any way of checking whether a collision occurs other than a data-error.Synthetic file-ids have only been used with unit tests so far - but they will be needed to add cache mechanisms to non-HDFS filesystems.In case of Synthetic file-ids, it is recommended that we track the full-tuple (path, mtime, len) in the cache so that a cache-hit for the synthetic file-id can be compared against the parameters &amp; only accepted if those match.
12996	TempTable Temp tables shouldn&apos;t be lockedInternally, INSERT INTO ... VALUES statements use temp table to accomplish its functionality. But temp tables shouldn&amp;apos;t be stored in the metastore tables for ACID, because they are by definition only visible inside the session that created them, and we don&amp;apos;t allow multiple threads inside a session. If a temp table is used in a query, it should be ignored by lock manager.mysql&gt; select * from COMPLETED_TXN_COMPONENTS;+-----------+--------------+-----------------------+------------------+| CTC_TXNID | CTC_DATABASE | CTC_TABLE             | CTC_PARTITION    |+-----------+--------------+-----------------------+------------------+|         1 | acid         | t1                    | NULL             ||         1 | acid         | values__tmp__table__1 | NULL             ||         2 | acid         | t1                    | NULL             ||         2 | acid         | values__tmp__table__2 | NULL             ||         3 | acid         | values__tmp__table__3 | NULL             ||         3 | acid         | t1                    | NULL             ||         4 | acid         | values__tmp__table__1 | NULL             ||         4 | acid         | t2p                   | ds=today         ||         5 | acid         | values__tmp__table__1 | NULL             ||         5 | acid         | t3p                   | ds=today/hour=12 |+-----------+--------------+-----------------------+------------------+
13063	UDF Create UDFs for CHR and REPLACECreate UDFS for these functions.CHR: convert n where n : [0, 256) into the ascii equivalent as a varchar. If n is less than 0 or greater than 255, return the empty string. If n is 0, return null.REPLACE: replace all substrings of &amp;apos;str&amp;apos; that match &amp;apos;search&amp;apos; with &amp;apos;rep&amp;apos;.Example. SELECT REPLACE(&amp;apos;Hack and Hue&amp;apos;, &amp;apos;H&amp;apos;, &amp;apos;BL&amp;apos;);Equals &amp;apos;BLack and BLue&amp;apos;"
13086	null LLAP: Programmatically initialize log4j2 to print out the properties locationIn some cases, llap daemon gets initialized with different log4j2.properties than the expected llap-daemon-log4j2.properties. It will be easier if programmatically configure log4j2 so that we can print out the location of properties file that is used for initialization.
13089	bug Rounding in Stats for equality expressionsCurrently we divide numRows(long) by countDistinct(long), thus ignoring the decimals. We should do proper rounding.This is specially useful for equality expressions over columns whose values are unique. As NDV estimates allow for a certain error, if countDistinct &gt; numRows, we end up with 0 rows in the estimate for the expression.
13093	Database hive metastore does not exit on start failureIf metastore startup fails for some reason, such as not being able to access the database, it fails to exit. Instead the process continues to be up in a bad state.This is happening because of a non daemon thread.
13108	Operator Partition Operators: SORT BY randomness is not safe with network partitionsSORT BY relies on a transient Random object, which is initialized once per deserialize operation.This results in complications during a network partition and when Tez/Spark reuses a cached plan.
13125	Database Support masking and filtering of rows/columnsTraditionally, access control at the row and column level is implemented through views. Using views as an access control method works well only when access rules, restrictions, and conditions are monolithic and simple. It however becomes ineffective when view definitions become too complex because of the complexity and granularity of privacy and security policies. It also becomes costly when a large number of views must be manually updated and maintained. In addition, the ability to update views proves to be challenging. As privacy and security policies evolve, required updates to views may negatively affect the security logic particularly when database applications reference the views directly by name. HIVE row and column access control helps resolve all these problems.
13144	Node HS2 can leak ZK ACL objects when curator retries to create the persistent ephemeral nodeWhen the node gets deleted from ZK due to connection loss and curator tries to recreate the node, it might leak ZK ACL.
13153	method method method SessionID is appended to thread name twiceHIVE-12249 added sessionId to thread name. In some cases the sessionId could be appended twice. Example log lineDEBUG [6432ec22-9f66-4fa5-8770-488a9d3f0b61 6432ec22-9f66-4fa5-8770-488a9d3f0b61 main]
13175	Compactor Disallow making external tables transactionalThe fact that compactor rewrites contents of ACID tables is in conflict with what is expected of external tables.Conversely, end user can write to External table which certainly not what is expected of ACID table.So we should explicitly disallow making an external table ACID.
13186	TableName ALTER TABLE RENAME should lowercase table name and hdfs location
13200	Table Partition SELECT TABLE TABLE ROW Aggregation functions returning empty rows on partitioned columnsRunning aggregation functions like MAX, MIN, DISTINCT against partitioned columns will return empty rows if table has property: &amp;apos;skip.header.line.count&amp;apos;=&amp;apos;1&amp;apos;Reproduce:DROP TABLE IF EXISTS test;CREATE TABLE test (a int)PARTITIONED BY (b int)ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;apos;|&amp;apos;TBLPROPERTIES(&amp;apos;skip.header.line.count&amp;apos;=&amp;apos;1&amp;apos;);INSERT OVERWRITE TABLE test PARTITION (b = 1) VALUES (1), (2), (3), (4);INSERT OVERWRITE TABLE test PARTITION (b = 2) VALUES (1), (2), (3), (4);SELECT * FROM test;SELECT DISTINCT b FROM test;SELECT MAX(b) FROM test;SELECT DISTINCT a FROM test;The output:0: jdbc:hive2://localhost:10000/default&gt; SELECT * FROM test;+---------+---------+--+| test.a  | test.b  |+---------+---------+--+| 2       | 1       || 3       | 1       || 4       | 1       || 2       | 2       || 3       | 2       || 4       | 2       |+---------+---------+--+6 rows selected (0.631 seconds)0: jdbc:hive2://localhost:10000/default&gt; SELECT DISTINCT b FROM test;+----+--+| b  |+----+--++----+--+No rows selected (47.229 seconds)0: jdbc:hive2://localhost:10000/default&gt; SELECT MAX(b) FROM test;+-------+--+|  _c0  |+-------+--+| NULL  |+-------+--+1 row selected (49.508 seconds)0: jdbc:hive2://localhost:10000/default&gt; SELECT DISTINCT a FROM test;+----+--+| a  |+----+--+| 2  || 3  || 4  |+----+--+3 rows selected (46.859 seconds)
13201	table table Compaction shouldn&apos;t be allowed on non-ACID tableLooks like compaction is allowed on non-ACID table, although that&amp;apos;s of no sense and does nothing. Moreover the compaction request will be enqueued into COMPACTION_QUEUE metastore table, which brings unnecessary overhead.We should prevent compaction commands being allowed on non-ACID tables.
13210	null Revert changes in HIVE-12994 related to metastoreAs we do not control what is written in the physical layer and thus we cannot ensure NULLS ORDER (and even if we did, currently we do not take advantage of it), it seems exposing the NULLS ORDER property at metastore level does not make much sense. We will revert that part of patch HIVE-12994.
13223	null HoS  may hang for queries that run on 0 splitsCan be seen on all timed out tests after HIVE-13040 went in
13226	Query Improve tez print summary to print query execution breakdownWhen tez print summary is enabled, methods summary is printed which are difficult to correlate with the actual execution time. We can improve that to print  the execution times in the sequence of operations that happens behind the scenes.Instead of printing the methods name it will be useful to print something like below1) Query Compilation time2) Query Submit to DAG Submit time3) DAG Submit to DAG Accept time4) DAG Accept to DAG Start time5) DAG Start to DAG End timeWith this it will be easier to find out where the actual time is spent.
13227	null LLAP: Change daemon initialization logs from INFO to WARNIn production LLAP is typically run with WARN log level. It will be useful to print the llap daemon initialization configs at WARN level instead of INFO level so that we can verify if daemon configs are propagated properly.NO PRECOMMIT TESTS
13233	Column Expression Use min and max values to estimate better stats for comparison operatorsWe should benefit from the min/max values for each column to calculate more precisely the number of rows produced by expressions with comparison operators
13236	null LLAP: token renewal interval needs to be set
13242	WindowingSpec DISTINCT keyword is dropped by the parser for windowingTo reproduce, the following query can be used:select distinct first_value(t) over ( partition by si order by i, b ) from over10k limit 100;The distinct keyword is ignored and duplicates are produced.
13247	null HIVE-13040 broke spark testsI confirmed that spark tests are getting stuck due to HIVE-13040. join_empty is an example test; it gets stuck on master presently, presumably because 0 splits are generated. When I reverted  HIVE-13040 locally, it passed for me. We should fix this or revert HIVE-13040
13251	HiveDecimal hive can&apos;t read the decimal in AVRO file generated from previous versionHIVE-7174 makes the avro schema change to match avro definition, while it breaks the compatibility if the file is generated from the previous Hive although the file schema from the file for such decimal is not correct based on avro definition. We should allow to read old file format "precision" : "4", "scale": "8", but when we write, we should write in the new format.
13258	null LLAP: Add hdfs bytes read and spilled bytes to tez print summaryWhen printing counters to console it will be useful to print hdfs bytes read and spilled bytes which will help with debugging issues faster.
13269	null Simplify comparison expressions using column stats
13286	method method query Query ID is being reused across queriesAihua Xu I see this commit made via HIVE-11488. I see that query id is being reused across queries. This defeats the purpose of a query id. I am not sure what the purpose of the change in that jira is but it breaks the assumption about a query id being unique for each query. Please take a look into this at the earliest.
13287	Filter operator Add logic to estimate stats for IN operatorCurrently, IN operator is considered in the default case: reduces the input rows to the half. This may lead to wrong estimates for the number of rows produced by Filter operators.
13291	null ORC BI Split strategy should consider block size instead of file sizeWhen we force split strategy to use "BI" (using hive.exec.orc.split.strategy), entire file is considered as single split. This might be inefficient when the files are large. Instead, BI should consider splitting at block boundary.
13320	null Apply HIVE-11544 to explicit conversions as well as implicit onesParsing 1 million blank values through cast(x as int) is 3x slower than parsing a valid single digit.
13327	method version bug SessionID added to HS2 threadname does not trim spacesHIVE-13153 introduced off-by-one in appending spaces to thread names.NO PRECOMMIT TESTS
13342	LlapDecider Improve logging in llap decider and throw exception in case llap mode is all but we cannot run in llap.Currently we do not log our decisions with respect to llap. Are we running everything in llap mode or only parts of the plan. We need more logging. Also, if llap mode is all but for some reason, we cannot run the work in llap mode, fail and throw an exception advise the user to change the mode to auto.
13343	null Need to disable hybrid grace hash join in llap mode except for dynamically partitioned hash joinDue to performance reasons, we should disable use of hybrid grace hash join in llap when dynamic partition hash join is not used. With dynamic partition hash join, we need hybrid grace hash join due to the possibility of skews.
13372	FunctionRegistry Hive Macro overwritten when multiple macros are used in one columnWhen multiple macros are used in one column, results of the later ones are over written by that of the first.For example:Suppose we have created a table called macro_test with single column x in STRING type, and with data as:"a""bb""ccc"We also create three macros:CREATE TEMPORARY MACRO STRING_LEN(x string) length(x);CREATE TEMPORARY MACRO STRING_LEN_PLUS_ONE(x string) length(x)+1;CREATE TEMPORARY MACRO STRING_LEN_PLUS_TWO(x string) length(x)+2;When we ran the following query,SELECTCONCAT(STRING_LEN(x), ":", STRING_LEN_PLUS_ONE(x), ":", STRING_LEN_PLUS_TWO(x)) aFROM macro_testSORT BY a DESC;We get result:3:3:32:2:21:1:1instead of expected:3:4:52:3:41:2:3Currently we are using Hive 1.2.1, and have applied both HIVE-11432 and HIVE-12277 patches.
13381	Timestamp Date Server Oracle Timestamp & date should have precedence in type hierarchy than string groupBoth sql server &amp; oracle treats date/timestamp higher in hierarchy than varchars
13400	service Following up HIVE-12481, add retry for Zookeeper service discovery
13412	Hive External table -  fields terminated by &apos;\u0044&apos; - 0044 is being interpreted as decimal and not hex, (comma) as the decimal value of &amp;apos;44&amp;apos; and hex value of &amp;apos;2c&amp;apos;In the following example I&amp;apos;m using  &amp;apos;\u0044&amp;apos; as delimiter which is being interpreted as comma.hive&gt; create external table test_delimiter_dec_unicode (c1 int,c2 int,c3 int) row format delimited fields terminated by &amp;apos;\u0044&amp;apos;;OKTime taken: 0.035 secondshive&gt; show create table test_delimiter_dec_unicode;OKCREATE EXTERNAL TABLE `test_delimiter_dec_unicode`(`c1` int,`c2` int,`c3` int)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &amp;apos;,&amp;apos;...
13422	CreateTableDesc DecimalTypeInfo ColumnStatsDesc Table FieldSchema HiveDecimal LoadTableDesc Analyse command not working for column having datatype as decimal(38,0)For the reprodrop table sample_test;CREATE TABLE IF NOT EXISTS sample_test( key decimal(38,0),b int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;apos;,&amp;apos; STORED AS TEXTFILE;load data local inpath &amp;apos;/home/hive/analyse.txt&amp;apos; into table sample_test;ANALYZE TABLE sample_test COMPUTE STATISTICS FOR COLUMNS;Sample data20234567894567498250824983000004 050320807548878498250695083000004 040120807548878498250687183000004 020120807548878498250667783000004 040120807548878496250656783000004 0
13428	bug description like ZK SM in LLAP should have unique paths per clusterNoticed this while working on some other patch
13439	Operation JDBC: provide a way to retrieve GUID to query Yarn ATSHIVE-9673 added support for passing base64 encoded operation handles to ATS. We should a method on client side to retrieve that.
13458	query Heartbeater doesn&apos;t fail query when heartbeat failsWhen a heartbeat fails to locate a lock, it should fail the current query. That doesn&amp;apos;t happen, which is a bug.Another thing is, we need to make sure stopHeartbeat really stops the heartbeat, i.e. no additional heartbeat will be sent, since that will break the assumption and cause the query to fail.
13485	null Session id appended to thread name multiple times.HIVE-13153 addressed a portion of this issue. Follow up from there.
13525	null HoS hangs when job is emptyObserved in local tests. This should be the cause of HIVE-13402.
13540	method hplsql method method decimal Casts to numeric types don&apos;t seem to work in hplsqlMaybe I&amp;apos;m doing this wrong? But it seems to be broken.Casts to string types seem to work fine, but not numbers.This code:temp_int     = CAST(&amp;apos;1&amp;apos; AS int);print temp_inttemp_float   = CAST(&amp;apos;1.2&amp;apos; AS float);print temp_floattemp_double  = CAST(&amp;apos;1.2&amp;apos; AS double);print temp_doubletemp_decimal = CAST(&amp;apos;1.2&amp;apos; AS decimal(10, 4));print temp_decimaltemp_string = CAST(&amp;apos;1.2&amp;apos; AS string);print temp_stringProduces this output:[vagrant@hdp250 hplsql]$ hplsql -f temp2.hplsqlwhich: no hbase in (/usr/lib64/qt-3.3/bin:/usr/lib/jvm/java/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/puppetlabs/bin:/usr/local/share/jmeter/bin:/home/vagrant/bin)WARNING: Use "yarn jar" to launch YARN applications.nullnullnullnull1.2The software I&amp;apos;m using is not anything released but is pretty close to the trunk, 2 weeks old at most.
13542	null Missing stats for tables in TPCDS performance regression suiteThese are the tables whose stats are missing in data/files/tpcds-perf/metastore_export/csv/TAB_COL_STATS.txt:catalog_returnscatalog_salesinventorystore_returnsstore_salesweb_returnsweb_salesThanks to Jesus Camacho Rodriguez for discovering this issue.
13553	null CTE with upperCase alias throws exception
13602	null TPCH q16 return wrong result when CBO is onRunning tpch with factor 2,q16 returns 1,160 rows when CBO is on,while returns 24,581 rows when CBO is off.See attachment for detail .
13618	Partition Mysql Derby Postgres Oracle Table Column Trailing spaces in partition column will be treated differentlyWe store the partition spec value in the metastore. In mysql (and derby i think), the trailing space is ignored. That is, if you have a partition column "col" (type varchar or string) with value "a " and then select from the table where col = "a", it will return. However, in postgres and Oracle, the trailing space is not ignored.
13619	null Bucket map join plan is incorrectSame as HIVE-12992. Missed a single line check. TPCDS query 4 with bucketing can produce this issue.
13628	bug Support for permanent functions - error handling if no restartSupport for permanent functions - error handling if no restart
13676	null Tests failing because metastore doesn&apos;t come upIn 5-6 test classes, metastore is required to be up for tests to run. The metastore is started in setup Phase asynchronously. But there&amp;apos;s no logic to wait till the metastore comes up. Hence, sometimes tests run even when metastore isn&amp;apos;t up and fail.
13701	task scheduler LLAP: Use different prefix for llap task scheduler metricsLLAP task scheduler runs inside AM and typically runs on different hosts than llap daemons. Using the same prefix "llapdaemon" for daemon metrics and task scheduler metrics will cause conflicts when these metrics are published with hadoop-metrics2.NO PRECOMMIT TESTS
13725	null ACID: Streaming API should synchronize calls when multiple threads use the same endpointCurrently, the streaming endpoint creates a metastore client which gets used for RPC. The client itself is not internally thread safe. Therefore, the API methods should provide the relevant synchronization so that the methods can be called from different threads. A sample use case is as follows:1. Thread 1 creates a streaming endpoint and opens a txn batch.2. Thread 2 heartbeats the txn batch.With the current impl, this can result in an "out of sequence response", since the response of the calls in thread1 might end up going to thread2 and vice-versa.
13730	Partition Avoid double spilling the same partition when memory threshold is set very lowI am seeing hybridgrace_hashjoin_1.q getting stuck on master.
13744	Hive Complex type column LLAP IO - add complex types supportRecently, complex type column vectors were added to Hive. We should use them in IO elevator.Vectorization itself doesn&amp;apos;t support complex types (yet), but this would be useful when it does, also it will enable LLAP IO elevator to be used in non-vectorized context with complex types after HIVE-13617
13749	Hive Memory leak in Hive MetastoreLooking a heap dump of 10GB, a large number of Configuration objects(&gt; 66k instances) are being retained. These objects along with its retained set is occupying about 95% of the heap space. This leads to HMS crashes every few days.I will attach an exported snapshot from the eclipse MAT.
13756	reducer Map failure attempts to delete reducer _temporary directory on multi-query pig queryA pig script, executed with multi-query enabled, that reads the source data and writes it as-is into TABLE_A as well as performing a group-by operation on the data which is written into TABLE_B can produce erroneous results if any map fails. This results in a single MR job that writes the map output to a scratch directory relative to TABLE_A and the reducer output to a scratch directory relative to TABLE_B.If one or more maps fail it will delete the attempt data relative to TABLE_A, but it also deletes the _temporary directory relative to TABLE_B. This has the unintended side-effect of preventing subsequent maps from committing their data. This means that any maps which successfully completed before the first map failure will have its data committed as expected, other maps not, resulting in an incomplete result set.
13809	BloomFilter ObjectCache Hybrid Grace Hash Join memory usage estimation didn&apos;t take into account the bloom filter sizeMemory estimation is important during hash table loading, because we need to make the decision of whether to load the next hash partition in memory or spill it. If the assumption is there&amp;apos;s enough memory but it turns out not the case, we will run into OOM problem.Currently hybrid grace hash join memory usage estimation didn&amp;apos;t take into account the bloom filter size. In large test cases (TB scale) the bloom filter grows as big as hundreds of MB, big enough to cause estimation error.The solution is to count in the bloom filter size into memory estimation.Another issue this patch will fix is possible NPE due to object cache reuse during hybrid grace hash join.
13813	Hive operation Add Metrics for the number of Hive operations waiting for compileCurrently, without hive.driver.parallel.compilation introduced in HIVE-4239, only one SQL operation can enter the compilation block per HS2 instance, and all the rest will be blocked. We should add metrics info for the number of operations that are blocked.
13832	bug Add missing license header to filesPreparing to cut the branch for 2.1.0.
13833	null Add an initial delay when starting the heartbeatSince the scheduling of heartbeat happens immediately after lock acquisition, it&amp;apos;s unnecessary to send heartbeat at the time when locks is acquired. Add an initial delay to skip this.
13838	BasicStats Set basic stats as inaccurate for all ACID tables
13840	OrcSplit OrcFile Orc split generation is reading file footers twiceRecent refactorings to move orc out introduced a regression in split generation. This leads to reading the orc file footers twice during split generation.
13859	UDF employee mask() UDF not retaining day and month field valuesFor date type parameters, mask() UDF replaces year/month/day field values with the values given in arguments to the UDF. Argument value -1 is treated as special, to specify that mask() should retain the value in the parameter. This allows to selectively mask only year/month/day fields.Specifying "-1" does not retain the values for day/month fields; however the year value is retained, as shown below.0: jdbc:hive2://localhost:10000&gt; select id, join_date from employee where id &lt; 4;+-----+-------------+--+| id  |  join_date  |+-----+-------------+--+| 1   | 2012-01-01  || 2   | 2014-02-01  || 3   | 2013-03-01  |+-----+-------------+--+3 rows selected (0.435 seconds)0: jdbc:hive2://localhost:10000&gt; select id, mask(join_date, -1, -1, -1, -1,-1, -1,-1,-1) join_date from employee where id &lt; 4;+-----+-------------+--+| id  |  join_date  |+-----+-------------+--+| 1   | 2012-01-01  || 2   | 2014-01-01  || 3   | 2013-01-01  |+-----+-------------+--+3 rows selected (0.344 seconds)
13927	bug Adding missing header to Java files
13934	Vertex Configure Tez to make nocondiional task size memory available for the ProcessorCurrently, noconditionaltasksize is not validated against the container size, the reservations made in the container by Tez for Inputs / Outputs etc.Check this at compile time to see if enough memory is available, or set up the vertex to reserve additional memory for the Processor.
13945	Decimal HiveDecimal HiveDecimalWritable HiveServer2 SelectOperator FilterOperator Table Column Decimal value is displayed as rounded when selecting where clause with that decimal value.Create a table withe a column of decimal type(38,18) and insert &amp;apos;4327269606205.029297&amp;apos;. Then select with that value displays its rounded value, which is 4327269606205.0293000000000000000: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&gt; drop table if exists test;No rows affected (0.229 seconds)0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&gt;0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&gt; create table test (dc decimal(38,18));No rows affected (0.125 seconds)0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&gt;0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&gt; insert into table test values (4327269606205.029297);No rows affected (2.372 seconds)0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&gt;0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&gt; select * from test;+-----------------------------------+--+|              test.dc              |+-----------------------------------+--+| 4327269606205.029297000000000000  |+-----------------------------------+--+1 row selected (0.123 seconds)0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&gt;0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&gt; select * from test where dc = 4327269606205.029297000000000000;+-----------------------------------+--+|              test.dc              |+-----------------------------------+--+| 4327269606205.029300000000000000  |+-----------------------------------+--+1 row selected (0.109 seconds)
13961	null ACID: Major compaction fails to include the original bucket files if there&apos;s no delta directoryThe issue can be reproduced by steps below:1. Insert a row to Non-ACID table2. Convert Non-ACID to ACID table (i.e. set transactional=true table property)3. Perform Major compaction
13971	null Address testcase failures of acid_globallimit.q and etc
13985	null ORC improvements for reducing the file system calls in task sideHIVE-13840 fixed some issues with addition file system invocations during split generation. Similarly, this jira will fix issues with additional file system invocations on the task side. To avoid reading footers on the task side, users can set hive.orc.splits.include.file.footer to true which will serialize the orc footers on the splits. But this has issues with serializing unwanted information like column statistics and other metadata which are not really required for reading orc split on the task side. We can reduce the payload on the orc splits by serializing only the minimum required information (stripe information, types, compression details). This will decrease the payload on the orc splits and can potentially avoid OOMs in application master (AM) during split generation. This jira also address other issues concerning the AM cache. The local cache used by AM is soft reference cache. This can introduce unpredictability across multiple runs of the same query. We can cache the serialized footer in the local cache and also use strong reference cache which should avoid memory pressure and will have better predictability.One other improvement that we can do is when hive.orc.splits.include.file.footer is set to false, on the task side we make one additional file system call to know the size of the file. If we can serialize the file length in the orc split this can be avoided.
13997	Hive HiveConf HiveException FileSinkOperator Utilities SessionState HiveMetaStoreClient HiveDriver HiveOperation HiveFileFormatUtils HiveOutputFormat HiveInputFormat HiveStorageHandler HiveStatsUtils HiveTxnManager HiveMetaHook HiveAuthorizationProvider HiveLockManager HiveLockObject HiveLockMode HiveDriverRunHook HiveDriverRunHookContext Insert overwrite directory doesn&apos;t overwrite existing filesCan be easily reproduced by running INSERT OVERWRITE DIRECTORY to the same dir twice.
14003	Hive queries running against llap hang at times - preemption issuesThe preemption logic in the Hive processor needs some more work. There are definitely windows where the abort flag is completely dropped within the Hive processor.
14013	Table Column Output Describe table doesn&apos;t show unicode properlyDescribe table output will show comments incorrectly rather than the unicode itself.hive&gt; desc formatted t1;# Detailed Table InformationTable Type:             MANAGED_TABLETable Parameters:COLUMN_STATS_ACCURATE   {\"BASIC_STATS\":\"true\"}comment                 \u8868\u4E2D\u6587\u6D4B\u8BD5numFiles                0
14027	Table Select Isnull NULL values produced by left outer join do not behave as NULLConsider the following setup:create table tbl (n bigint, t string);insert into tbl values (1, &amp;apos;one&amp;apos;);insert into tbl values(2, &amp;apos;two&amp;apos;);select a.n, a.t, isnull(b.n), isnull(b.t) from (select * from tbl where n = 1) a  left outer join  (select * from tbl where 1 = 2) b on a.n = b.n;1    one    false    trueThe query should return true for isnull(b.n).I&amp;apos;ve tested by inserting a row with null value for the bigint column into tbl, and isnull returns true in that case.
14060	null Hive: Remove bogus "localhost" from Hive splitsOn remote filesystems like Azure, GCP and S3, the splits contain a filler location of "localhost".This is worse than having no location information at all - on large clusters yarn waits upto 200[1] seconds for heartbeat from "localhost" before allocating a container.To speed up this process, the split affinity provider should scrub the bogus "localhost" from the locations and allow for the allocation of "*" containers instead on each heartbeat.[1] - yarn.scheduler.capacity.node-locality-delay=40 x heartbeat of 5s
14062	bug Changes from HIVE-13502 overwritten by HIVE-13566Appears that changes from HIVE-13566 overwrote the changes from HIVE-13502. I will confirm with the author that it was inadvertent before I re-add it. Thanks
14073	null update config whiltelist for sql std authorizationNew configs that should go in security whitelist have been added. Whitelist needs updating.
14074	FunctionRegistry RELOAD FUNCTION should update dropped functionsDue to HIVE-2573, functions are stored in a per-session registry and only loaded in from the metastore when hs2 or hive cli is started. Running RELOAD FUNCTION in the current session is a way to force a reload of the functions, so that changes that occurred in other running sessions will be reflected in the current session, without having to restart the current session. However, while functions that are created in other sessions will now appear in the current session, functions that have been dropped are not removed from the current session&amp;apos;s registry. It seems inconsistent that created functions are updated while dropped functions are not.
14126	null With ranger enabled, partitioned columns is returned first when you execute select star
14132	null Don&apos;t fail config validation for removed configsUsers may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.
14133	null Don&apos;t fail config validation for removed configsUsers may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.
14135	Beeline beeline output not formatted correctly for large column widthsIf the column width is too large then beeline uses the maximum column width when normalizing all the column widths. In order to reproduce the issue, run set -v;Once the configuration variables is classpath which can be extremely large width (41k characters in my environment).
14141	Beeline Fix for HIVE-14062 breaks indirect urls in beelineLooks like the patch for HIVE-14062 breaks indirect urls which uses environment variables to get the url in beelineIn order to reproduce this issue:$ export BEELINE_URL_DEFAULT="jdbc:hive2://localhost:10000"$ beeline -u default
14144	FunctionRegistry MetaStoreUtils HiveMetaStore FunctionInfo Hive Function Permanent functions are showing up in show functions, but describe says it doesn&apos;t exist
14146	Table Metadata Column Column comments with "\n" character "corrupts" table metadataCreate a table with the following(noting the \n in the COMMENT):CREATE TABLE commtest(first_nm string COMMENT &amp;apos;Indicates First name\nof an individual);Describe shows that now the metadata is messed up:beeline&gt; describe commtest;+-------------------+------------+-----------------------+--+|     col_name      | data_type  |        comment        |+-------------------+------------+-----------------------+--+| first_nm             | string       | Indicates First name  || of an individual  | NULL       | NULL                  |+-------------------+------------+-----------------------+--+
14147	Hive Hive PPD might remove predicates when they are defined as a simple expr e.g. WHERE &apos;a&apos;
14164	Driver Connection JDBC: Add retry in JDBC driver when reading config values from ZKSometimes ZK may intermittently experience network partitioning. During this time, clients trying to open a JDBC connection get an exception. To improve user experience, we should implement a retry logic and fail after retrying.
14174	Table Fix creating buckets without scheme informationIf a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.
14175	Table Fix creating buckets without scheme informationIf a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.
14176	null CBO nesting windowing function within each other when merging Project operatorsThe translation into a physical plan does not support this way of expressing windowing functions. Instead, we will not merge the Project operators when we find this pattern.
14194	null Investigate optimizing the query compilation of long or-list in where statementThe following query will take long time to compile if the where statement has a long list of &amp;apos;or&amp;apos;. Investigate if we can optimize it.select * from srcwhere key = 1or key =2or ....
14236	Select CTAS with UNION ALL puts the wrong stats in Tezto repo. in Tez, create table t as select * from src union all select * from src;
14238	User Ownership shouldn&apos;t be checked if external table location doesn&apos;t existWhen creating external table with SQL authorization, we require RWX permission + ownership of the table location. If the location doesn&amp;apos;t exist, we check on parent dir (recursively), which means we require the user owns everything under parent dir. I think this is not necessary - we don&amp;apos;t have to check ownership of parent dir, or we just check non-recursively.
14248	SemanticAnalyzer UnionOperator Table query with view in union adds underlying table as direct inputIn the following case,create view V as select * from T;select * from V union all select * from VThe semantic analyzer inputs contain input table T as a direct input instead of adding it as an indirect input.
14251	Table Select Table Union All of different types resolves to incorrect datacreate table src(c1 date, c2 int, c3 double);insert into src values (&amp;apos;2016-01-01&amp;apos;,5,1.25);select * from(select c1 from src union allselect c2 from src union allselect c3 from src) t;It will return NULL for the c1 values. Seems the common data type is resolved to the last c3 which is double.
14254	bug description command Hive Correct the hive version by changing "svn" to "git"When running "hive --version", "subversion" is displayed below, which should be "git".$ hive --versionHive 2.1.0-SNAPSHOTSubversion git://
14308	null While using column stats estimated data size may become 0Found during a run of HIVE-12181
14311	task Query Task scheduler No need to schedule Heartbeat task if the query doesn&apos;t require locksOtherwise the Heartbeat task will just stay there and not be cleaned up, which may cause OOM eventually.
14324	null ORC PPD for floats is brokenORC stores min/max stats, bloom filters by passing floats as doubles using java&amp;apos;s widening conversion. So if we write a float value of 0.22 to ORC file, the min/max stats and bloom filter will use 0.2199999988079071 double value.But when we do PPD, SARG creates literals by converting float to string and then to double which compares 0.22 to 0.2199999988079071 and fails PPD evaluation.hive&gt; create table orc_float (f float) stored as orc;hive&gt; insert into table orc_float values(0.22);hive&gt; set hive.optimize.index.filter=true;hive&gt; select * from orc_float where f=0.22;OKhive&gt; set hive.optimize.index.filter=false;hive&gt; select * from orc_float where f=0.22;OK0.22This is not a problem for doubles and decimals.This issue was introduced in HIVE-8460 but back then there was no strict type check when SARGs are created and also PPD evaluation does not convert to column type. But now predicate leaf creation in SARG enforces strict type check for boxed literals and predicate type and PPD evaluation converts stats and constants to column type (predicate).
14326	JoinOperator Merging outer joins without conditions can lead to wrong resultsHIVE-13069 enabled cartesian product merging. However, merge should only be performed between INNER joins.
14345	Beeline HiveServer2 Beeline result table has erroneous charactersBeeline returns query results with erroneous characters. For example:0: jdbc:hive2://xxxx:10000/def&gt; select 10;+------+--+| _c0  |+------+--+| 10   |+------+--+1 row selected (3.207 seconds)
14346	bug Change the default value for hive.mapred.mode to nullHIVE-12727 introduces three new configurations to replace the existing hive.mapred.mode, which is deprecated. However, the default value for the latter is &amp;apos;nonstrict&amp;apos;, which prevent the new configurations from being used (see comments in that JIRA for more details).This proposes to change the default value for hive.mapred.mode to null. Users can then set the three new configurations to get more fine-grained control over the strict checking. If user want to use the old configuration, they can set hive.mapred.mode to strict/nonstrict.
14349	UDFLike Vectorization: LIKE should anchor the regexesRLIKE works like contains() and LIKE works like matches().The UDFLike LIKE -&gt; Regex conversion returns unanchored regexes making the vectorized LIKE behave like RLIKE.create temporary table x (a string) stored as orc;insert into x values(&amp;apos;XYZa&amp;apos;), (&amp;apos;badXYZa&amp;apos;);select * from x where a LIKE &amp;apos;XYZ%a%&amp;apos; order by 1;OKXYZabadXYZaTime taken: 4.029 seconds, Fetched: 2 row(s)
14367	Type Estimated size for constant nulls is 0since type is incorrectly assumed as void.
14387	Beeline Add an option to skip the table names for the column headersIt would be good to have an option where the beeline output could skip reporting the &lt;table_name&gt;.&lt;column_name&gt; in the headers.Eg:0: jdbc:hive2://:&gt; select * from sample_07 limit 1;--------------------------------------------------------------------------------------------------sample_07.code	sample_07.description	sample_07.total_emp	sample_07.salary--------------------------------------------------------------------------------------------------00-0000	Operations	123	12345--------------------------------------------------------------------------------------------------b) After the option is set:0: jdbc:hive2://:&gt; select * from sample_07 limit 1;---------------------------------------------------code	 description	total_emp	 salary---------------------------------------------------00-0000	Operations	123	12345
14390	null Wrong Table alias when CBO is onThere are 5 web_sales references in query95 of tpcds ,with alias ws1-ws5.But the query plan only has ws1 when CBO is on.query95 :SELECT count(distinct ws1.ws_order_number) as order_count,sum(ws1.ws_ext_ship_cost) as total_shipping_cost,sum(ws1.ws_net_profit) as total_net_profitFROM web_sales ws1JOIN customer_address ca ON (ws1.ws_ship_addr_sk = ca.ca_address_sk)JOIN web_site s ON (ws1.ws_web_site_sk = s.web_site_sk)JOIN date_dim d ON (ws1.ws_ship_date_sk = d.d_date_sk)LEFT SEMI JOIN (SELECT ws2.ws_order_number as ws_order_numberFROM web_sales ws2 JOIN web_sales ws3ON (ws2.ws_order_number = ws3.ws_order_number)WHERE ws2.ws_warehouse_sk &lt;&gt; ws3.ws_warehouse_sk) ws_wh1ON (ws1.ws_order_number = ws_wh1.ws_order_number)LEFT SEMI JOIN (SELECT wr_order_numberFROM web_returns wrJOIN (SELECT ws4.ws_order_number as ws_order_numberFROM web_sales ws4 JOIN web_sales ws5ON (ws4.ws_order_number = ws5.ws_order_number)WHERE ws4.ws_warehouse_sk &lt;&gt; ws5.ws_warehouse_sk) ws_wh2ON (wr.wr_order_number = ws_wh2.ws_order_number)) tmp1ON (ws1.ws_order_number = tmp1.wr_order_number)WHERE d.d_date between &amp;apos;2002-05-01&amp;apos; and &amp;apos;2002-06-30&amp;apos; andca.ca_state = &amp;apos;GA&amp;apos; ands.web_company_name = &amp;apos;pri&amp;apos;;
14397	null Queries ran after reopening of tez session launches additional sessionsSay we have configured hive.server2.tez.default.queues with 2 queues q1 and q2 with default expiry interval of 5 mins.After 5 mins of non-usage the sessions corresponding to queues q1 and q2 will be expired. When new set of queries are issue after this expiry, the default sessions backed by q1 and q2 and reopened again. Now when we run more queries the reopened sessions are not used instead new session is opened.At this point there will be 4 sessions running (2 abandoned sessions and 2 current sessions).
14435	Vectorizer Vectorization: missed vectorization for const varchar()2016-08-05T09:45:16,488  INFO [main] physical.Vectorizer: Failed to vectorize2016-08-05T09:45:16,488  INFO [main] physical.Vectorizer: Cannot vectorize select expression: Const varchar(1) fThe constant throws an illegal argument because the varchar precision is lost in the pipeline.
14479	JoinOperator HiveConf HiveMetaStoreClient HiveTxnManager HiveDriver HiveSessionImpl Add some join tests for acid table
14566	Timestamp method method method method method bug LLAP IO reads timestamp wronglyHIVE-10127 is causing incorrect results when orc_merge12.q is run in llap.It reads timestamp wrongly.LLAP IO Enabledhive&gt; select atimestamp1 from alltypesorc3xcols limit 10;OK1969-12-31 15:59:46.674NULL1969-12-31 15:59:55.7871969-12-31 15:59:44.1871969-12-31 15:59:50.4341969-12-31 16:00:15.0071969-12-31 16:00:07.0211969-12-31 16:00:04.9631969-12-31 15:59:52.1761969-12-31 15:59:44.569LLAP IO Disabledhive&gt; select atimestamp1 from alltypesorc3xcols limit 10;OK1969-12-31 15:59:46.674NULL1969-12-31 15:59:55.7871969-12-31 15:59:44.1871969-12-31 15:59:50.4341969-12-31 16:00:14.0071969-12-31 16:00:06.0211969-12-31 16:00:03.9631969-12-31 15:59:52.1761969-12-31 15:59:44.569
14581	UDF Add chr udfhttp://docs.aws.amazon.com/redshift/latest/dg/r_CHR.html
14648	file LLAP: Avoid private pages in the SSD cacheThere&amp;apos;s no reason for the SSD cache to have private mappings to the cache file, there&amp;apos;s only one reader and the memory overheads aren&amp;apos;t worth it.
14652	Table Partition Select incorrect results for not in on partition columnscreate table foo (i int) partitioned by (s string);insert overwrite table foo partition(s=&amp;apos;foo&amp;apos;) select cint from alltypesorc limit 10;insert overwrite table foo partition(s=&amp;apos;bar&amp;apos;) select cint from alltypesorc limit 10;select * from foo where s not in (&amp;apos;bar&amp;apos;);No results. IN ... works correctly
14677	Beeline HiveConnection HiveStatement HiveDriver Beeline should support executing an initial SQL script
14697	null Can not access kerberized HS2 Web UIFailed to access kerberized HS2 WebUI with following error msg:curl -v -u : --negotiate http://util185.phx2.cbsig.net:10002/&gt; GET / HTTP/1.1&gt; Host: util185.phx2.cbsig.net:10002&gt; Authorization: Negotiate YIIU7...[redacted]...&gt; User-Agent: curl/7.42.1&gt; Accept: */*&gt;&lt; HTTP/1.1 413 FULL head&lt; Content-Length: 0&lt; Connection: close&lt; Server: Jetty(7.6.0.v20120127)It is because the Jetty default request header (4K) is too small in some kerberos case.So this patch is to increase the request header to 64K.
14726	DatabaseConnection delete statement fails when spdo is on
14751	Date Add support for date truncationAdd support for floor (&lt;time&gt; to &lt;timeunit&gt;), which is equivalent to date_trunc(&lt;timeunit&gt;, &lt;time&gt;).https://www.postgresql.org/docs/9.1/static/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC
14783	null bucketing column should be part of sorting for delete/update operation when spdo is on
14805	Hive Subquery inside a view will have the object in the subquery as the direct inputHere is the repro steps.create table t1(col string);create view v1 as select * from t1;create view dataview as select * from  (select * from v1) v2;select * from dataview;If hive is configured with authorization hook like Sentry, it will require the access not only for dataview but also for v1, which should not be required.The subquery seems to not carry insideview property from the parent query.
14865	bug Fix comments after HIVE-14350there are still some comments in the code that should&amp;apos;ve been updated in HIVE-14350
14873	UDF UDFDayOfWeek Add UDF for extraction of &apos;day of week&apos;
14943	bug Base ImplementationCreate the 1st pass functional implementation of MERGEThis should run e2e and produce correct results.
14959	SemanticAnalyzer method Fix DISTINCT with windowing when CBO is enabled/disabledFor instance, the following query with CBO off:select distinct last_value(i) over ( partition by si order by i ),first_value(t)  over ( partition by si order by i )from over10k limit 50;will fail, with the following message:SELECT DISTINCT not allowed in the presence of windowing functions when CBO is off
14984	bug Hive-WebUI access results in Request is a replay (34) attackWhen trying to access kerberized webui of HS2, The following error is receivedGSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))While this is not happening for RM webui (checked if kerberos webui is enabled)To reproduce the issueTry runningcurl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://&lt;hostname&gt;:10002/from any cluster nodesorTry accessing the URL from a VM with windows machine and firefox browser to replicate the issueThe following workaround helped, but need a permanent solution for the bugWorkaround:=========First access the index.html directly and then actual URL of webuicurl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://&lt;hostname&gt;:10002/index.htmlcurl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://&lt;hostname&gt;:10002In browser:First accesshttp://&lt;hostname&gt;:10002/index.htmlthenhttp://&lt;hostname&gt;:10002
15029	Filter operator Add logic to estimate stats for BETWEEN operatorCurrently, BETWEEN operator is considered in the default case: reduces the input rows to the half. This may lead to wrong estimates for the number of rows produced by Filter operators.
15030	null Fixes in inference of collation for Tez cost modelTez cost model might get NPE if collation returned by join algorithm is null.
15061	null Metastore types are sometimes case sensitiveImpala recently encountered an issue with the metastore (IMPALA-4260 ) where column stats would get dropped when adding a column to a table.The reason seems to be that Hive does a case sensitive check on the column stats types during an "alter table" and expects the types to be all lower case. This case sensitive check doesn&amp;apos;t appear to happen when the stats are set in the first place.We&amp;apos;re solving this on the Impala end by storing types in the metastore as all lower case, but Hive&amp;apos;s behavior here is very confusing. It should either always be case sensitive, so that you can&amp;apos;t create column stats with types that Hive considers invalid, or it should never be case sensitive.
15137	null metastore add partitions background thread should use current usernameThe background thread used in HIVE-13901 for adding partitions needs to be reinitialized with current UGI for each invocation. Otherwise the user in context while thread was created would be the current UGI during the actions in the thread.
15233	UDF UDF UUID() should be non-deterministicThe UUID() function should be non-deterministic.
